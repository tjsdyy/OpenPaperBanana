{
  "metadata": {
    "name": "curated",
    "description": "Curated reference set of methodology diagrams from 13 academic papers. Each figure was visually verified to be a methodology/architecture diagram.",
    "version": "2.0.0",
    "source": "Manual visual inspection of MinerU-parsed PDFs",
    "categories": [
      "agent_reasoning",
      "vision_perception",
      "generative_learning",
      "science_applications"
    ],
    "total_examples": 13
  },
  "examples": [
    {
      "id": "2404.15806v1",
      "source_context": "4 Methodology\n\nThis section presents the model architecture of StructMAE in detail. First, we provide a detailed introduction to the masking module in GMAE (§4.1). Subsequently, a comprehensive exploration $( \\ S 4 . 2 )$ of the masking strategy is provided. Then, the details of the proposed StrucMAE are elucidated (§4.3). Finally, the overall StructMAE architecture, encompassing training and inference details, is expounded (§4.4).\n\n4.1 Introducing the GMAE Masking Module\n\nPrior to processing graph data within the GMAE encoder, a subset of nodes $\\mathcal { V } _ { \\mathrm { m a s k } } \\subset \\mathcal { V }$ is sampled for masking. According to the methodology described in [Hou et al., 2022], the features of these sampled nodes are replaced with a learnable vector $\\pmb { x } _ { [ \\mathbf { M } ] } \\in \\mathbb { R } ^ { d }$ . Accordingly, for a node $\\nu _ { i }$ within the masked node subset $\\boldsymbol { \\mathcal { N } } _ { \\mathrm { m a s k } }$ , its feature $\\widetilde { \\mathbf { X } } _ { i }$ in the altered feature matrix $\\widetilde { \\mathbf { X } }$ is defined as follows:\n\n[Equation: $$\n\\widetilde {\\mathbf {x}} _ {i} = \\left\\{ \\begin{array}{l l} \\mathbf {x} _ {[ \\mathrm {M} ]} & v _ {i} \\in \\mathcal {V} _ {\\text {m a s k}} \\\\ \\mathbf {x} _ {i} & v _ {i} \\notin \\mathcal {V} _ {\\text {m a s k}} \\end{array} \\right.. \\tag {2}\n$$]\n\nRegarding the approach for selecting nodes to mask, most existing works [Hou et al., 2022; Li et al., 2023] utilize a random masking strategy. This method assigns an equal masking probability of masking to each node within a graph. A more detailed exploration of the random masking strategy will be presented in the following section (§4.2).\n\n4.2 Reconsidering the Random Masking Strategy\n\nIn GMAE context, the masking strategy is crucial, as it significantly influences the type of information that the model learns. In previous studies [Hou et al., 2022], nodes within a graph are masked randomly, each with an equal probability. However, this strategy overlooks the varying structural information of different nodes, which has been shown to be crucial in graph learning tasks. Therefore, we conduct a preliminary\n\nexperiment to explore whether the incorporation of structure priors could augment pre-training efficacy.\n\nIn the experiment, models are pre-trained on the MNIST dataset and evaluated in unsupervised settings. Nodes forming numerical values are identified as those possessing rich structural information. Subsequently, the masking probability for these structurally informative nodes is manually increased. The results, presented on the left side of Figure 3, reveal the impact of this modified masking strategy on unsupervised accuracy. The results indicate that: 1) A marginal increase in the masking probability for nodes with rich structural information enhances the model’s pre-training learning. Furthermore, up to a probability threshold of 0.2, a corresponding gradual increase in the model’s accuracy is observed. 2) Conversely, excessively raising the masking probability of these structurally significant nodes detrimentally affects the model’s training. These findings corroborate our initial discussion and highlight the importance of proposing an effective method to integrate structural information into the masking process.\n\n4.3 The Proposed Masking Strategy\n\nInspired by the preceding discussion, we introduce an innovative structure-guided masking approach for GMAE, named StructMAE (as depicted in Figure 2 (b)). StructMAE involves integrating the graph’s structural knowledge into the masking process, thereby directing the model’s learning trajectory more effectively. StructMAE is composed of two primary elements: Structure-based Scoring (SBS) and Structureguided Masking (SGM).\n\nStructure-based Scoring\n\nThe SBS evaluates the significance of each node based on its structural role within the graph. This evaluation facilitates the identification of nodes that are pivotal for the model to learn, allowing for a more targeted masking approach. To determine the importance of nodes, we introduce two distinct methodologies: the pre-defined and learnable methods.\n\nPre-defined Structure-based Scoring. The predefined method involves using a set of predetermined criteria, based on the known structural information, to evaluate node importance. Specifically, the computation of importance score $\\mathbf { S } \\in \\mathbb { R } ^ { n }$ is achieved using the PageRank algorithm [Page et al., 1999], a well-established technique for evaluating node significance based on graph structures. Thus, the importance\n\nscore of node $\\nu _ { i }$ is defined as:\n\n[Equation: $$\ns _ {i} = \\frac {1 - e}{n} + e \\sum_ {j \\in \\mathcal {N} _ {i}} \\frac {s _ {j}}{L _ {j}}, \\tag {3}\n$$]\n\nwhere $e$ denotes the damping factor, $L _ { i }$ represents the degree of node $\\nu _ { i }$ , and $N _ { i }$ denotes the set of neighboring nodes of node $\\nu _ { i }$ . In addition to PageRank, other prevalent metrics for assessing node importance comprise degree, closeness centrality, and betweenness centrality. Each of these methods is based on different underlying principles, offering diverse perspectives on a node’s role and influence within a graph. Although these methods constitute more intricate ways of evaluating node importance, our empirical findings suggest that PageRank serves as a straightforward and effective measure. A detailed discussion and comparison of these methods is presented in the following section.\n\nLearnable Structure-based Scoring. In contrast to the predefined method, the learnable approach dynamically assesses node significance based on the evolving state of the graph during the learning process. Specifically, it integrates the formulation of the assessment metric with masked graph modeling, enabling end-to-end learning of this metric. To accomplish this, we employ a lightweight scoring network, denoted as $f _ { S } ( \\cdot )$ , which assesses the importance of each node $\\nu _ { i }$ . The scoring network’s design is similar to a GNN-style layer, and it effectively captures graph structural information. Thus, the importance score $s _ { i } \\in \\mathbf S$ for node $\\nu _ { i }$ is calculated as follows:\n\n[Equation: $$\ns _ {i} = \\text {S i g m o i d} \\left(f _ {S} \\left(\\mathbf {x} _ {i}, \\mathbf {A}\\right)\\right), \\quad i = 1, \\dots , n. \\tag {4}\n$$]\n\nA higher score $s _ { i }$ indicates greater importance of the corresponding node $\\nu _ { i }$ . Following the scoring, node features are sorted in descending order based on these scores. The ordered node features and their respective scores are represented as $\\left\\{ { \\bf x } _ { i } ^ { \\prime } \\right\\}$ and $\\left\\{ { { s } _ { i } ^ { \\prime } } \\right\\}$ , respectively, where $i = 1 , \\ldots , n$ . To facilitate learning the scoring network $f _ { S } ( \\cdot )$ , the predicted scores are multiplied by the node features to serve as modulating factors. This operation is formally expressed as:\n\n[Equation: $$\n\\hat {\\mathbf {X}} = \\left\\{\\mathbf {x} _ {i} ^ {\\prime \\prime} \\mid \\mathbf {x} _ {i} ^ {\\prime \\prime} = \\mathbf {x} _ {i} ^ {\\prime} * s _ {i} ^ {\\prime} \\right\\}, \\quad i = 1, \\dots , n, \\tag {5}\n$$]\n\nwhere $\\hat { \\mathbf { X } }$ denotes the set of node features after scoring. This mechanism ensures that the scoring network is continually updated and refined throughout the model’s training process.\n\nIn this section, we explore the two distinct SBS methods proposed for StructMAE. First, the pre-defined SBS method offers simplicity and is particularly effective when the structural characteristics are well-understood and can be explicitly defined beforehand. Conversely, the learnable SBS is more adaptable and can cater to complex and variable graph structures, making it suitable for scenarios where the node significance cannot be easily predetermined.\n\nStructure-guided Masking\n\nThe SGM component utilizes the scores generated by the SBS to guide its masking decisions. It selectively and progressively masks nodes in an easy-to-hard manner, thereby enhancing the model’s capacity to effectively learn and represent graph structures. Specifically, in the initial training stage, a subset of easy nodes with lower scores is masked, making it easier for\n\nthe model to predict them using basic neighboring information. As training progresses, the masking strategy evolves to encompass more challenging nodes. This enables the model to capture intricate structural information and, consequently, enhances its learning capabilities.\n\nThis masking strategy relies on the importance scoring matrix S. It is hypothesized that nodes with higher S scores are more informative and significant. Consequently, we gradually increase the masking probabilities for these high-scoring nodes during masked graph modeling. To implement this, we rank the nodes based on their scores $\\mathbf { S } = \\{ s _ { 1 } , s _ { 2 } , \\cdots , s _ { n } \\}$ and identify the top $K$ indices that constitute the set $_ y$ . The masking probability for each node is then determined as:\n\n[Equation: $$\n\\gamma_ {i} = \\epsilon + \\left\\{ \\begin{array}{l l} \\beta & v _ {i} \\in \\mathcal {Y} \\\\ 0 & v _ {i} \\notin \\mathcal {Y} \\end{array} \\right., \\tag {6}\n$$]\n\nwhere $\\epsilon$ denotes random noise drawn from a uniform distribution $( \\epsilon \\sim U ( 0 , 1 ) )$ and $\\beta$ represents the increased probability assigned to nodes with higher scores. In this instance, nodes in the set $_ y$ are considered more informative, and consequently, the model is anticipated to prioritize these nodes. The number of masked nodes, denoted by $K$ , is dynamically adjusted throughout the training process. Initially set at zero, $K$ progressively increases with epoch according to the following formula:\n\n[Equation: $$\nK (t) = p \\cdot n \\cdot \\sqrt {t / T}, \\tag {7}\n$$]\n\nwhere $K ( t )$ represents the $K$ value at epoch ??, ?? denotes the total number of nodes in the graph, $p$ is the predefined mask ratio, and $T$ represents the total number of training epochs. This approach enables the model to progressively concentrate on more challenging nodes, thereby enhancing its acquisition of complex structural information.\n\n4.4 Overall StructMAE Architecture\n\nTraining Process. The StructMAE training process begins with an input graph from which a specified proportion of nodes is chosen based on our selective masking strategy. The selected nodes are subsequently masked using a masktoken. The graph, now with partially masked features, is subsequently fed into the encoder, which generates encoded representations of the nodes. After that, the decoder module is responsible for predicting and reconstructing the features of masked nodes. For optimization, we adopt the scaled cosine error as utilized in GraphMAE [Hou et al., 2022].\n\nInference and Downstream Tasks. StructMAE is designed to cater to two distinct downstream applications: unsupervised and transfer learning. In unsupervised learning, the encoder processes the input graph without masking during the inference stage. The node embeddings generated by the encoder are then utilized for graph classification tasks with linear classifiers or support vector machines. In the transfer learning context, the pre-trained models are fine-tuned on different datasets. This fine-tuning enables the model to adjust to new data domains, leveraging the foundational knowledge gained during its initial training on the source dataset. Each of these downstream tasks emphasizes the versatility and applicability of StructMAE in diverse graph learning scenarios.",
      "caption": "Overview of StructMAE. (a) The overall pipeline: Input Graph -> SBS (Structure-based Scoring) -> SGM (Structure-guided Masking) -> Enc -> Dec -> Reconstructed Nodes. (b) SBS module with pre-defined/learnable scoring and TopK selection. (c) SGM module showing progressive masking across training epochs.",
      "image_path": "images/2404.15806v1.jpg",
      "category": "science_applications",
      "aspect_ratio": 3.24,
      "source_paper": "2404.15806v1"
    },
    {
      "id": "2601.03570v1",
      "source_context": "2 Dataset Construction\n\nWe follow previous studies (Wang et al., 2024c) and define a concept (e.g., dog) as an abstraction over the world that captures the shared features and essential characteristics of a class of entities. Conceptual knowledge (e.g., a dog has four legs) refers to factual or relational information associated with a concept, reflecting familiarity with and understanding of its properties and relations (Wang et al., 2024a). Each concept is therefore linked to a set of conceptual knowledge that collectively describe its semantic content. To study concept learning across different levels of concreteness, we consider both concrete concepts that are grounded in direct sensory experience (e.g., dog), and abstract concepts which are not directly perceptible (e.g., love). We sample 500 concrete concepts from the THINGS dataset (Hebart et al., 2023) and 500 abstract concepts from the Concreteness Ratings dataset (Brysbaert et al., 2014). For each concept, we retrieve associated conceptual knowledge from ConceptNet (Speer et al., 2017), a largescale knowledge graph of millions of concepts and 34 typed relations. To improve interpretability, we consolidate these relations into five high-level knowledge types, as shown in Appendix A.\n\nTo mitigate influence of pre-existing knowledge encoded in LLMs, we replace each real concept name with one fictional name generated by GPT-5 (Hurst et al., 2024), while preserve associated conceptual knowledge, as shown in Figure 1(A), to form our FICO, FIctional COncept dataset, as detailed in Section B in Appendix. Following prior work (Zucchet et al., 2025b), we use GPT-5 to produce natural-language templates for each relation type (e.g., “{concept} has the ability to” for CapableOf ), converting knowledge triples (e.g., (dog, CapableOf, run)) into prefix–target training examples. For evaluation, we sample 500 concepts and use a disjoint template pool to construct test instances with novel surface forms, enabling assessment of whether LLMs learns underlying relations rather than memorizing templates.\n\n3 How Internal Concept Representations Correlate with LLM Learning and Forgetting Dynamics?\n\n3.1 Concept Circuit as Internal Concept Representation\n\nPrior work (Yao et al., 2025) models a pretrained LLM as a directed acyclic graph (DAG), where nodes correspond to computational components in\n\nthe forward pass (e.g., neurons, attention heads, embeddings), and edges capture their interactions (e.g., residual connections, attention operations, and linear projections). Given a knowledge triple $k _ { i j } = ( c _ { i } , r _ { i j } , o _ { i j } )$ , where $c _ { i }$ is a subject concept (e.g., dog), $r _ { i j }$ is a relation type (e.g., HasProperty), and $o _ { i j }$ is an object which can be another concept or a descriptive phrase (e.g., fleas or four legs), a Knowledge Circuit is defined as the minimal computational subgraph that can faithfully predict the target object $o _ { i j }$ conditioned on a textual prefix converted from the subject–relation pair $( c _ { i } , r _ { i j } )$ (Yao et al., 2025). We adapt this notion to concept-level analysis and define a Concept Circuit for concept $c _ { i }$ as the computational subgraph that can faithfully predict all conceptual knowledge $\\{ k _ { i 0 } , k _ { i 1 } , . . . , k _ { i j } , . . . \\}$ associated with $c _ { i }$ , thereby serving as an internal representation of the concept within the model’s parametric memory.\n\nTo extract concept circuits at different checkpoints during continual training, we use EAP-IG (Hanna et al., 2024) as our circuit identification method. EAP-IG assigns an importance score to each edge while balancing computational efficiency with attribution faithfulness1. Given the edge importance scores, we construct a circuit by selecting the top-scoring edges such that the resulting subgraph preserves at least $70 \\%$ of the full model’s performance on the corresponding concept.\n\nGraph Metrics for Concept Circuits To characterize the structure of concept circuits and their relationship to concept learning dynamics in LLMs, we compute four families of standard graph-theoretic metrics: (1) Node Importance, measured as the standard deviation of eigenvector centrality (Newman, 2010), which quantifies how unevenly structural influence is distributed across nodes. Higher variance indicates a more concentrated hub structure, which may facilitate concept acquisition but increase vulnerability to interference or forgetting. (2) Redundancy, measured by density (Newman, 2010), defined as the ratio of existing edges to the maximum possible number of edges. Higher density reflects more redundant connections. (3) Information Flow Efficiency, measured by global efficiency (Latora and Marchiori, 2001), i.e., the average inverse shortest-path distance between node pairs. Higher global efficiency indicates that signals can propagate more efficiently across the cir-\n\ncuit. (4) Robustness, measured by the average $k$ -core number (Seidman, 1983), which captures the depth of a circuit’s densely connected core and is used as a proxy for resilience to disruption.\n\n3.2 Experiment Design\n\nTo quantify how internal concept representations (i.e., concept circuits) relate to concept acquisition and forgetting, we formalize learning and forgetting at the knowledge-triple and concept level.\n\nDefinition 1 (Knowledge Learning/Forgetting Degree). For a conceptual knowledge triple $\\overline { { \\textit { k } = } }$ $\\overline { { ( c , r , o ) } }$ , where c is the subject concept, r is the relation, and o is the target object, the knowledge learning degree is defined as the increase in the logit2 assigned to o after training relative to before training, given a textual prefix constructed from $( c , r )$ , while the knowledge forgetting degree is defined as the decrease in the logit assigned to o after continued training.\n\nDefinition 2 (Concept Learning/Forgetting Degree). For a concept c with associated conceptual knowledge $\\{ k _ { 0 } , k _ { 1 } , \\ldots \\}$ , the concept learning (forgetting) degree is defined as the average knowledge learning (forgetting) degree across its knowledge triples: ${ \\frac { 1 } { | \\{ k _ { i } \\} | } } \\sum _ { i } \\phi ( k _ { i } )$ , where $\\phi ( k _ { i } )$ denotes the learning (forgetting) degree of triple $k _ { i }$ .\n\nExperiment Setup. Given a pre-trained LLM $\\pi _ { 0 }$ , we conduct a two-stage continual pre-training: (1) Stage 1 (Concept Acquisition): The model is continually trained on the training set of FICO to learn novel concepts, yielding a new model $\\pi _ { 1 }$ . This stage is designed to support the analysis of concept learning dynamics based on $\\pi _ { 0 }$ and $\\pi _ { 1 }$ ; (2) Stage 2 (Forgetting Induction): Based on $\\pi _ { 1 }$ from Stage 1, we further train the model on the BIO dataset (Allen-Zhu and Li, 2023), an standard pretraining dataset used in previous LLMs knowledge acquisition studies (Allen-Zhu and Li, 2023; Zucchet et al., 2025a) , yielding a new model $\\pi _ { 2 }$ . We then analyze concept forgetting dynamics using $\\pi _ { 1 }$ and $\\pi _ { 2 }$ . We experiment with two open-source LLMs: GPT-2 Large 3 (Radford et al., 2019) and LLaMA-3.2-1B-Instruct 4 5 (Dubey et al., 2024). For both training stages, we concatenate the prefix\n\nand target phrase, as described in Section 2 and optimize the model using the next-token prediction. For evaluation, we only provide the prefix and ask the model to generate an appropriate target phrase on the FICO test set. We use Spearman’s correlation coefficient (Spearman, 1961) to analyze the correlation between concept learning and forgetting dynamics and concept circuit topology.\n\n3.3 Experiment Finding\n\nFinding 1. Concepts exhibit substantial heterogeneity in learning degree and forgetting degree, indicating that LLMs acquire and forget different concepts to markedly different extents under the same training regime.\n\nConcept Learning/Forgetting Degree Distribution. Figure 2 illustrates the distribution of concept learning degrees (logit increase) and forgetting degree (logit decrease) across 500 concepts on the FICO test set. The distribution is unimodal but widely spread, indicating pronounced variability in how effectively different concepts are learned or forgotten. Practically, this variability implies that some concepts are learned more readily and robustly, whereas others require greater training effort to achieve comparable learning and to mitigate forgetting. This observation motivates our subsequent analysis, which seeks to identify indicators that can account for the differing learning and forgetting behaviors across concepts.\n\nFinding 2. Concept learning degree and forgetting degree shows non-trivial, statistically significant correlationsa with multiple circuit graph metrics, suggesting that circuit structure can serve as an informative indicator of concept learning and forgetting dynamics.\n\naThe observed Spearman correlations are statistically significant (Conover, 1999) $( p < 0 . 0 0 1 )$ ).\n\nCorrelation between Concept Learning/Forgetting and Concept Circuits. Figures 3a and 3b\n\nexamine how learning and forgetting degrees relate to the structural properties of concept circuits, as characterized by four graph metrics. We observe non-trivial and consistent Spearman correlations, indicating that circuit topology is systematically associated with how concepts are acquired and retained. For concept learning, positive correlations with metrics capturing node importance and robustness, such as eigenvector centrality and $k$ -core, suggest that circuits with centralized bottlenecks and stable structural cores tend to achieve stronger logit gains. Similarly, positive correlations with circuit density and global efficiency indicate that structural redundancy and integrated information flow, characterized by multiple pathways and shorter distances between components, can reinforce learning signals and facilitate concept acquisition.\n\nNotably, these same structural properties also contribute to vulnerability during continual training. Forgetting degree is positively correlated with\n\nnode importance, indicating that circuits dominated by a small set of influential hub nodes are more vulnerable to forgetting when these components are perturbed by subsequent training. Surprisingly, higher circuit density, larger $k$ -core depth, and greater global efficiency, properties that benefit learning, are likewise associated with increased forgetting. One possible explanation is that highly redundant and tightly interconnected circuits entangle concept representations more strongly with other knowledge, amplifying interference during continued training. Together, these results reveal a structural trade-off: while centralized, dense, and robust circuit organizations favor rapid and effective learning, more modular circuit structures may be better suited for mitigating forgetting under continual training. More broadly, these findings suggest that circuit-level graph metrics can serve as informative indicators of concept learning dynamics in LLMs, offering insights for concept-aware continual pretraining decisions such as allocating training effort for newly introduced concepts and previously learned concepts.\n\nFigure 4 shows the evolution of circuit graph metrics during the forgetting stage. LLMs exhibit a consistent stage-wise temporal pattern across\n\nmost graph metrics: an initial increase followed by a gradual decrease and eventual stabilization. This behavior is observed across all structural aspects of concept circuits, suggesting that forgetting is accompanied by systematic circuit reorganization rather than monotonic structural decay. The early increase may indicates transient entanglement of previously learned concept circuits with newly introduced knowledge during continued training, while the subsequent decrease and stabilization reflect structural relaxation and convergence to a weaker representation under interference.\n\nFinding 4. Concepts with larger learning degree tend to exhibit larger forgetting degree during subsequent training, indicating that knowledge acquired more aggressively is often less stable and more susceptible to interference.\n\nCorrelation between Concept Learning and Forgetting. Figure 5 illustrates a positive association between learning degree and forgetting degree under continual training. That is, concepts that achieve larger gains during acquisition also tend to degrade more when the model is later trained on new data. Combined with the circuit analyses above, these results suggest a structural trade-off. During learning, concepts with larger gains are often supported by circuits that are more integrated and strongly connected, which can enable coordinated updates across concept-related components. However, the same integration may increase overlap with subsequently trained knowledge. When influence is concentrated in a small number of hubs (high eigenvector-centrality variance), perturbations to these hubs can induce circuit-wide changes, making such concepts more fragile under continued training. Overall, the learn–forget correlation suggests that stronger acquisition does not necessarily imply better consolidation, and that concept representations that are easy to strengthen may also be easier to disrupt. This observation naturally raises the question of whether such vulnerability arises in isolation or is also shaped by interactions among concurrently learned concepts.",
      "caption": "Concept Circuits: A transformer block (Attention, FFN, Add&Norm layers) maps to a concept circuit graph. Graph metrics (Node Importance, Info Flow Efficiency, Redundancy, Robustness) characterize internal concept representations.",
      "image_path": "images/2601.03570v1.jpg",
      "category": "agent_reasoning",
      "aspect_ratio": 0.61,
      "source_paper": "2601.03570v1"
    },
    {
      "id": "2601.05110v1",
      "source_context": "3 Methodology\n\n3.1 Problem Formulation\n\nWe consider a collaborative inference framework involving a large, high-capacity reasoning model (LLM, $M _ { L }$ ) and a small computationally efficient model (SLM, $M _ { S }$ ). Large reasoning models typically output a reasoning process $\\tau$ , often encapsulated within a <think> . . . </think> tag, followed by the final answer $A$ . Our framework focuses on accelerating the reasoning process $\\tau$ , while the final answer $A$ is always generated by $M _ { L }$\n\nto ensure correctness. Formally, the reasoning process is decomposed into a sequence of steps $\\mathcal { T } = \\{ s _ { 1 } , . . . , s _ { K } \\}$ based on structural delimiters (e.g., double newlines (Pan et al., 2025; Yang et al., 2025b)). Each step $s _ { k }$ consists of a sequence of tokens $s _ { k } = ( t _ { 1 } , \\ldots , t _ { L } )$ and is generated conditioned on the preceding context $\\mathbf { c } _ { k }$ , which includes the original question and all previous generated steps $s _ { 1 } , \\ldots , s _ { k - 1 }$ . The objective of collaborative inference is to dynamically assign each reasoning step $s _ { k }$ to $M _ { S }$ or $M _ { L }$ , minimizing overall inference latency while preserving the quality of reasoning required for accurate solutions.\n\n3.2 Overview\n\nTo navigate the efficiency-accuracy trade-off, we propose GlimpRouter, a training-free, step-aware collaboration strategy that routes reasoning steps based on the initial token entropy $( \\bf H _ { \\mathrm { i n i t } } )$ . Figure 3 illustrates the overall pipeline. Given an input question, the system operates in a step-wise manner. At the onset of each step, instead of blindly generating the full content, GlimpRouter employs $M _ { S }$ to “glimpse” the first token, yielding an entropy $\\mathbf { H } _ { \\mathrm { i n i t } }$ that quantizes the difficulty of the upcoming step: If $\\mathbf { H } _ { \\mathrm { i n i t } }$ falls below a threshold, the step is deemed routine, and the small model is responsible for continuing the generation. Otherwise, the step signals a cognitive pivot, and the context is handed over to a large model for high-quality generation. This “Probe-then-Dispatch” mechanism ensures that heavy computational resources are allocated solely for critical reasoning steps. Each\n\nstep is introduced in the following sections. The detailed procedure is outlined in Algorithm 1 in Appendix A.\n\n3.3 Glimpse: Initial Token Probing\n\nAt the beginning of the reasoning step $k$ , given the preceding context $\\mathbf { c } _ { k }$ , the small model $M _ { S }$ is invoked to predict the probability distribution of only the first token $t _ { k , 1 }$ . We compute the entropy ${ \\bf { H } } _ { \\mathrm { { i n i t } } } ( s _ { k } )$ of this distribution as a proxy for the cognitive uncertainty of the current reasoning step. This probing operation incurs a marginal computational cost equivalent to decoding a single token, unlike methods that necessitate generating an entire reasoning step (typically $L \\gg 1$ tokens) before verification. Even in scenarios where the step is subsequently identified as difficult and routed to $M _ { L }$ (thereby discarding the probe), this 1-token overhead is negligible compared to the substantial sunk costs associated with discarding fully generated invalid steps.\n\n3.4 Dynamic Model Routing\n\nGiven the entropy of the initial token in the current step, GlimpRouter dispatches the generation task to the appropriate model based on a threshold $\\tau$ :\n\nDelegate $( \\mathbf { H _ { i n i t } } ( s _ { k } ) \\leq \\tau )$ : Low entropy suggests that the small model is confident in the logical progression, and the step is likely routine. Consequently, $M _ { S }$ continues to autoregressively generate the remainder of step $s _ { k }$ until the delimiter is reached. This decision reduces the total cost by maximizing the utilization of the small model.\n\nIntervene $( \\mathbf { H _ { i n i t } } ( s _ { k } ) > \\tau )$ : High entropy indicates logical ambiguity or high cognitive load. In this scenario, $M _ { L }$ is selected to generate the step $s _ { k }$ . While this incurs a higher computational cost, it leverages the superior reasoning and inherent selfcorrection capabilities of LRMs (Guo et al., 2025). Specifically, $M _ { L }$ can rectify potential logical drifts accumulated in the historical context $\\mathbf { c } _ { k }$ , thereby satisfying the quality constraint. We provide a qualitative analysis of this implicit self-correction behavior in Appendix F.\n\n3.5 Efficient Model Switching\n\nA critical requirement for a step-level collaboration system, where every step involves context transitions, is minimizing the model switching overhead. To minimize the system overhead, we leverage prefix caching mechanisms supported by inference engines (Kwon et al., 2023; Zheng et al., 2024).\n\nWhen routing a request between models, the context $\\mathbf { c } _ { k }$ , which comprises the question and historical steps, is largely resident in the KV cache from previous interactions. Thus, the context processing is reduced to a highly parallelizable prefill phase rather than a serial re-computation. The resulting switching latency is comparable to decoding a few tokens, ensuring that the computational savings from $M _ { S }$ are not negated by routing overheads.\n\n3.6 Hierarchical Acceleration\n\nA distinct advantage of GlimpRouter lies in its step-level granularity. This coarse-grained design is inherently orthogonal to token-level optimizations, allowing our framework to be seamlessly integrated with various low-level acceleration techniques to achieve compound speedups. To maximize system throughput, we implement a hierarchical acceleration strategy in Section 4.3. At the inter-step level, GlimpRouter acts as a global planner, assigning routine logical steps to $M _ { S }$ to bypass the expensive $M _ { L }$ entirely. At the intra-step level, when $M _ { L }$ is invoked, we further accelerate its generation using Speculative Decoding (Leviathan et al., 2023). Specifically, we employ a “Draftthen-Verify” pipeline where the small draft model $M _ { S }$ proposes token sequences that are verified in parallel by $M _ { L }$ .",
      "caption": "Overview of GlimpRouter. The framework coordinates three layers: LLM (generates full steps and final answers), GlimpRouter (entropy-based routing checking initial token entropy H_init), and SLM (generates first tokens and complete steps). High H_init routes to LLM; low H_init stays with SLM.",
      "image_path": "images/2601.05110v1.jpg",
      "category": "agent_reasoning",
      "aspect_ratio": 2.95,
      "source_paper": "2601.05110v1"
    },
    {
      "id": "2601.05144v1",
      "source_context": "3 METHODOLOGY\n\n3.1 ALGORITHMIC REALIZATION OF PSV CONSTRUCTION $f _ { \\eta }$\n\nThis construction is primarily realized through Critical Tokens (CTs). Consequently, this section is organized into two main parts: the first details the method for identifying CTs, and the second explains how these tokens are utilized to construct PSV, corresponding to the function $f _ { \\eta }$ in Eq. 1.\n\n3.1.1 CRITICALITY SCORE\n\nTranslating Theorem 2.2 into a practical algorithm, we devise a Criticality Score for each word $w \\in \\mathcal V$ . The proof of this translation’s validity is discussed in detail in Appendix ??. This score is a composite measure reflecting both the global causal influence and the local competitive persistence of $w$ .\n\nGlobal Causal Contribution (GCC). This component aims to approximate $D _ { \\mathrm { c a u s a l } } ( w \\Vert \\theta )$ (Eq. equation 3) by quantifying a word $w$ ’s capacity to indirectly shape the reasoning trajectory through sustained high probability in causally interconnected steps. The GCC is formulated as:\n\n[Equation: $$\n\\operatorname {G C C} (w) = \\sum_ {i = 1} ^ {N} \\left[ P _ {i} (w) \\cdot \\lambda_ {i} \\cdot \\sum_ {j = i + 1} ^ {M} \\alpha_ {i \\rightarrow j} \\cdot P _ {j} (w) \\right] \\tag {5}\n$$]\n\nThe weight $\\lambda _ { i } = \\mathbf { J S } ( P _ { i } \\Vert P _ { i - 1 } )$ captures the magnitude of change in the models predictive distribution from step $i - 1$ to $i$ . A large JS divergence signals a critical juncture in the reasoning process, amplifying the contribution of words prominent at such points. The term αi→j = $\\begin{array} { r } { \\alpha _ { i  j } = \\frac { \\mathrm { S i m } \\bar { ( } P _ { i } ^ { \\cdot } , P _ { j } ) } { \\sum _ { k \\prime = 1 } ^ { N } \\mathrm { S i m } ( P _ { i } , P _ { k \\prime } ) } } \\end{array}$ represents the normalized semantic influence of the distributional state at step $i$ on that of step $j$ . Here, $\\begin{array} { r } { \\mathrm { S i m } ( P _ { i } , P _ { j } ) = \\frac { \\mathbf { P } _ { i } \\cdot \\mathbf { P } _ { j } } { \\| \\mathbf { P } _ { i } \\| \\| \\mathbf { P } _ { j } \\| } } \\end{array}$ is the cosine similarity between the vector representations of probability distributions $P _ { i }$ and $P _ { j }$ . This factor ensures that the influence of early critical steps is appropriately propagated and weighted in assessing a words contribution to later stages of reasoning.\n\nCompetitive Persistence Scoring (CPS). This component approximates $\\mathbb { E } _ { j > i } [ \\Delta S _ { i  j } ( w ) ]$ (from Eq. equation 4) by rewarding words that not only feature prominently in competitive generation contexts but also maintain this prominence over subsequent steps. The CPS for a word $w$ is calculated as:\n\n[Equation: $$\n\\operatorname {C P S} (w) = \\sum_ {i = 1} ^ {N} \\left[ S \\left(t _ {i}\\right) ^ {- 1} \\cdot \\left(1 - \\Delta_ {i} (w)\\right) \\cdot \\sum_ {j = i + 1} ^ {M} \\mathbb {I} \\left(w \\in \\operatorname {t o p} _ {k} \\left(P _ {j}\\right)\\right) \\right] \\tag {6}\n$$]\n\nThe term $S ( t _ { i } ) ^ { - 1 } = ( - \\log P _ { i } ( t _ { i } ) ) ^ { - 1 }$ inversely weights the contribution by the surprisal of the token $t _ { i }$ actually generated at step $i$ . This rewards contexts where the model makes a high-confidence choice, suggesting that such choices are more deliberate and impactful. The core of this reward lies\n\nin $\\Delta _ { i } ( w )$ , which measures the competitive pressure surrounding $w$ at step $i$ :\n\n[Equation: $$\n\\Delta_ {i} (w) = \\left\\{ \\begin{array}{l l} \\left| L _ {i} (w) - \\max  _ {v \\neq w} L _ {i} (v) \\right|, & \\text {i f} w = t _ {i} (\\text {i . e .}, w \\text {w a s s e l e c t e d}) \\\\ \\left| L _ {i} (w) - L _ {i} \\left(t _ {i}\\right) \\right|, & \\text {i f} w \\in \\operatorname {t o p} _ {k} \\left(L _ {i}\\right) \\text {a n d} w \\neq t _ {i} (\\text {i . e .}, w \\text {w a s a c l o s e c o m p e t i t o r}) \\\\ 1, & \\text {o t h e r w i s e (n o t c o m p e t i t i v e)} \\end{array} \\right. \\tag {7}\n$$]\n\nWhen $w$ is the selected token $t _ { i }$ , $\\Delta _ { i } ( w )$ is its logit margin over the strongest competitor. If $w$ was a top- $\\mathbf { \\nabla } \\cdot \\mathbf { k }$ candidate but not selected, $\\Delta _ { i } ( w )$ is its logit difference from the winner $t _ { i }$ . A smaller $\\Delta _ { i } ( w )$ indicates more intense competition. The reward thus assigns higher rewards to tokens that emerge from, or are central to, highly contested selection points.\n\n$\\begin{array} { r } { \\sum _ { j = i + 1 } ^ { M } \\mathbb { I } ( w \\in \\mathsf { t o p } _ { k } ( P _ { j } ) ) ; } \\end{array}$ )counts the number of times $w$ appears among the top- $k$ probability candidates in the $M - i$ steps immediately following step i. This serves as empirical validation of ws enduring relevance and high-frequency consideration throughout the local reasoning window, reinforcing its status as a critical element.\n\nConsolidated Criticality Score (CS). The final score synergistically combines these two aspects to provide a holistic measure of a token’s importance.\n\n[Equation: $$\n\\operatorname {C S} (w) = \\operatorname {G C C} (w) \\cdot \\log (1 + \\operatorname {C P S} (w)) \\tag {8}\n$$]\n\nThe set of Critical Tokens $\\scriptstyle { \\mathcal { C } } ^ { \\prime }$ , is then formed by selecting the $K$ tokens with the highest CS values, as Fig 1.II illustrated, providing the semantic anchors for the next stage of our methodology. The case study in Appendix H examines the distribution of normalized CS for CTs on different datasets, revealing their correspondence with the input and output of the model.\n\n3.1.2 FROM CRITICAL TOKENS TO PRINCIPAL SEMANTIC VECTOR\n\nWhile the discrete set $\\mathcal { C } ^ { \\prime }$ identifies key semantic anchors, it falls short of capturing the holistic, relational logic inherent in complex reasoning. To overcome this limitation, we transform this discrete set of tokens into a continuous vector representation, the PSV, that encapsulates the dominant semantic direction of the entire thinking phase.\n\nLet $E ( \\cdot )$ be the model’s token embedding function. We first construct an embedding matrix $H \\in$ $\\mathbb { R } ^ { K \\times d }$ by stacking the embeddings of the $K$ identified Critical Tokens from $\\scriptstyle { \\mathcal { C } } ^ { \\prime }$ , where $d$ is the embedding dimension.\n\n[Equation: $$\nH = \\left[ E \\left(w _ {1}\\right), E \\left(w _ {2}\\right), \\dots , E \\left(w _ {K}\\right) \\right] ^ {T}, \\quad \\forall w _ {i} \\in \\mathcal {C} ^ {\\prime} \\tag {9}\n$$]\n\nWe then apply Principal Component Analysis (PCA) to $H$ . The first principal component $\\mathbf { v } _ { 1 }$ , represents the direction of maximum variance within the embeddings of the most critical tokens. This direction captures their most significant shared semantic properties and reflects the primary axis of the model’s reasoning. We define the initial PSV $\\mathcal { R } _ { 0 }$ as this first principal component:\n\n[Equation: $$\n\\mathcal {R} _ {0} = \\mathbf {v} _ {1} = \\operatorname {P C A} _ {1} (H) \\tag {10}\n$$]\n\nThis initial PSV $\\mathcal { R } _ { 0 }$ , described in Definition 2.1 and Fig. 1.III, acts as a global semantic compass, providing a stable, overarching directional guide for the subsequent answering phase.\n\n3.2 SEMANTICALLY-ADAPTIVE WATERMARK EMBEDDING $g _ { \\sigma }$ AND $f _ { \\mu }$\n\nOur framework departs from conventional methods that employ a fixed watermark strength (Kirchenbauer et al., 2023b), inspired by Wang et al. (2025b). Instead, we introduce a semantically-adaptive mechanism where the watermark’s intensity is dynamically modulated based on the alignment of candidate tokens with the current PSV. This allows for a strong watermark signature on semantically coherent tokens while minimizing interference with the model’s natural generation process.\n\nDynamic Watermark Strength. At each generation step $i$ in the answer phase, we partition the vocabulary $\\nu$ into a green list $\\nu _ { g }$ and a red list $\\mathcal { V } _ { r }$ based on a standard cryptographic hash of the previous token, following Kirchenbauer et al. (2023b). However, instead of applying a fixed bonus $\\delta$\n\nto the logits of all green-list tokens, we compute a token-specific bonus $\\delta _ { i , w }$ for each candidate token $w \\in \\mathcal { V } _ { g }$ . This bonus is proportional to the token’s semantic relevance to the current PSV $\\mathcal { R } _ { i - 1 }$ :\n\n[Equation: $$\ns _ {w, i} = \\frac {E (w) \\cdot \\mathcal {R} _ {i}}{\\| E (w) \\| \\| \\mathcal {R} _ {i} \\|}, \\quad \\delta_ {i, w} = \\delta_ {0} + \\delta_ {\\lambda} \\cdot s _ {w, i - 1} \\tag {11}\n$$]\n\nwhere $s _ { w , i - 1 }$ is the cosine similarity between the embedding of token $v$ and the PSV $\\mathcal { R } _ { i - 1 }$ . $\\delta _ { 0 }$ is a base watermark strength, and $\\delta _ { \\lambda }$ is a scaling factor that controls the sensitivity to semantic alignment. The logit for a green-list token $w$ is then modified as $L _ { i } ( w ) \\gets L _ { i } ( w ) + \\bar { \\delta } _ { i , w }$ . This ensures that green-list tokens that are highly aligned with the intended reasoning trajectory receive a stronger watermark, reinforcing logical consistency. If a highly probable, contextually appropriate token falls into the red list, the relatively lower bonuses on green-list alternatives prevent significant quality degradation. The effect of these two parameters on model performance is analyzed in Section 4.5.\n\nDynamic PSV Update. The PSV is not static; it evolves with the generation of the answer to act as a semantic compass, tracking the local semantic context. After a token $t _ { i }$ is generated at step $i$ , we update the PSV using an exponential moving average:\n\n[Equation: $$\n\\mathcal {R} _ {i} = \\left(1 - \\beta_ {i}\\right) \\mathcal {R} _ {i - 1} + \\beta_ {i} E \\left(t _ {i}\\right), \\quad \\text {w h e r e} \\beta_ {i} = \\beta_ {\\text {b a s e}} \\cdot s _ {t _ {i}, i - 1} \\tag {12}\n$$]\n\nThe update rate $\\beta _ { i }$ is itself adaptive, depending on the semantic contribution of the newly generated token, where $\\beta _ { \\mathrm { b a s e } } \\in [ 0 , 1 ]$ is a small base learning rate, which is also analyzed in Section 4.5. This mechanism ensures that the PSV gradually incorporates the semantic content of the unfolding answer, allowing for smooth topical transitions while remaining anchored to the initial reasoning established in $T _ { \\mathrm { t h i n k } }$ .\n\nWatermark Detection. A significant advantage of our approach is that the detection process requires no modification to standard procedures in Kirchenbauer et al. (2023b). Despite the dynamic nature of the watermark embedding, detection remains stateless and does not require access to the PSV or the original prompt. It is performed using the same statistical z-test as in KGW by checking for a statistically significant bias towards green-list tokens in the generated text. The performance gain of the algorithm is attributed to its ability to identify a larger set of valid green tokens from the candidate list at each step $i$ , thereby reducing the number of red tokens.",
      "caption": "ReasonMark pipeline: A prompt is processed by RLLM with <think> reasoning. Criticality Score CS(w) = GCC(w) + log(1 + CPS(w)) identifies Top-K Critical Tokens. The PSV R_i is initialized with PCA and guides semantically-aligned watermark embedding.",
      "image_path": "images/2601.05144v1.jpg",
      "category": "generative_learning",
      "aspect_ratio": 0.91,
      "source_paper": "2601.05144v1"
    },
    {
      "id": "2601.06411v1",
      "source_context": "3 Methodology\n\nThe SEEM framework transforms a continuous stream of interaction passages into a hierarchical memory architecture composed of two complementary layers. The Episodic Memory Layer (EML) focuses on capturing the narrative progression by extracting and fusing structured Episodic Event Frames (EEFs) while the Graph Memory Layer (GML) organizes static factual relations into a structured relational graph. Both layers are grounded in original passages through a system of provenance pointers, which maintain the link between abstract memory units and their raw passage. During inference, these layers are integrated through a hybrid retrieval process utilizing the Reverse Provenance Expansion (RPE) mechanism to reconstruct a coherent and logically consistent context.\n\n3.1 Problem Formulation\n\nThe task of memory-augmented generation in longterm interactions is defined as follows. Given a chronological sequence of interaction passages $\\mathcal { P } = \\{ p _ { 1 } , p _ { 2 } , . . . , p _ { T } \\}$ , where each passage $p _ { t }$ represents a discrete unit of historical context, and a current user query $q \\in \\mathcal { Q }$ , the objective is to generate a response $a$ that is factually consistent with $\\mathcal { P }$ and contextually relevant to $q$ .\n\nWe formulate this problem as the optimization of a conditional probability $\\textstyle P ( a \\mid q , \\mathcal { P } )$ . Due to the significant length and semantic density of $\\mathcal { P }$ , the task requires the construction of an intermediate memory representation $\\mathcal { M }$ to bridge the gap between historical evidence and current reasoning. The process is decomposed into two core stages:\n\nMemory Consolidation. We define a transformation function $\\Phi : \\mathcal { P }  \\mathcal { M }$ that maps the raw interaction sequence into a structured representation space $\\mathcal { M }$ . This stage is designed to preserve essential thematic and relational information while mitigating the noise inherent in raw text.\n\nConditioned Generation. A retrieval augmented generation function $G ( q , \\mathcal { M } ) \\to a$ is employed to identify a relevant subset $\\mathcal { M } _ { s u b } \\subseteq \\mathcal { M }$ based on the query $q$ , leading to the final response generation:\n\n[Equation: $$\na = \\arg \\max  _ {a ^ {\\prime}} P \\left(a ^ {\\prime} \\mid q, \\mathcal {M} _ {s u b}; \\theta\\right) \\tag {1}\n$$]\n\nwhere $\\theta$ denotes the parameters of the underlying generative model. Here, the core challenge lies in designing a representation space $\\mathcal { M }$ that can effectively encode the narrative continuity and factual\n\ndependencies within $\\mathcal { P }$ . The system must ensure that the transition from $\\mathcal { P }$ to $\\mathcal { M }$ maintains provenance, allowing the final generation process to be grounded in the original source evidence.\n\n3.2 Episodic Memory Generation and Fusion\n\nTo maintain a coherent understanding of long-term interactions, we introduce a structured episodic memory layer. Instead of storing raw interaction turns, we transform a sequence of passages $\\mathcal { P } = \\{ p _ { 1 } , p _ { 2 } , . . . , p _ { T } \\}$ into discrete, event-centric units. As illustrated in Figure 1, this process consists of two phases: (1) extracting structured episodic event frames from each passage and (2) performing associative consolidation to merge related frames.\n\n3.2.1 Episodic Event Frame Extraction\n\nWe treat each passage $p _ { t }$ as a source signal to be instantiated into a cognitive frame. Following the principles of frame semantics (Fillmore, 1976), an EEF $\\mathbf { e } _ { t }$ encapsulates the structured semantics of $p _ { t }$ . We employ an LLM-based agent, $\\mathcal { F } _ { \\mathrm { e x t } }$ , to parse $p _ { t }$ into granular semantic roles and a high-level summary. To ensure the abstract memory remains grounded, each frame is linked back to its source passage via a provenance pointer ρemlt . The formal $\\rho _ { t } ^ { e m l }$ definition is:\n\n[Equation: $$\n\\begin{array}{l} \\mathbf {e} _ {t} = \\mathcal {F} _ {\\text {e x t}} \\left(p _ {t}; \\theta\\right) \\\\ = \\left\\langle \\rho_ {t} ^ {e m l}, v _ {\\text {s u m}}, \\left\\{\\left\\langle v _ {\\text {p a r}}, v _ {\\text {a c t}}, v _ {\\text {t m p}}, \\right. \\right. \\right. \\tag {2} \\\\ \\left. v _ {\\mathrm {s p a}}, v _ {\\mathrm {c a u}}, v _ {\\mathrm {m a n}} \\right\\rangle^ {(k)} \\left. \\right\\} _ {k = 1} ^ {N _ {t}} \\Bigg \\rangle \\\\ \\end{array}\n$$]\n\nwhere $v _ { \\mathrm { s u m } }$ is the event summary, and the subsequent components represent semantic roles: Participants $( v _ { \\mathrm { p a r } } )$ , Action $( v _ { \\mathrm { a c t } } )$ , Time $( v _ { \\mathrm { t m p } } )$ , Location $( v _ { \\mathrm { s p a } } )$ , Causality $( v _ { \\mathrm { c a u } } )$ , and Manner $( v _ { \\mathrm { m a n } } )$ . This hierarchical structure allows the agent to navigate memory through both thematic abstractions and precise textual anchors.\n\n3.2.2 Associative Consolidation and Fusion\n\nTo mitigate memory fragmentation, we implement an associative fusion mechanism that merges related observations into coherent scenes. When generating a new candidate frame $\\mathbf { e } _ { t }$ , the system retrieves the most relevant historical frame eprev and uses an LLM-based judge to determine if they belong to the same event:\n\n[Equation: $$\n\\delta_ {t} \\leftarrow \\mathcal {F} _ {\\text {j u d g e}} \\left(\\mathbf {e} _ {t}, \\mathbf {e} _ {\\text {p r e v}} \\mid \\text {p r o m p t} _ {\\text {s i m}}\\right) \\tag {3}\n$$]\n\nIf $\\delta _ { t } = 1$ , the integration agent ${ \\mathcal { F } } _ { \\mathrm { f u s e } }$ performs an associative merge, synthesizing the attributes of both frames and updating the summary $v _ { \\mathrm { s u m } }$ . Note that we aggregate their provenance pointers, updating $\\rho _ { t } ^ { e m l }$ to point to the union of all involved source passages. This ensures that a single consolidated frame can later serve as an entry point to all relevant evidence scattered across different turns.\n\n3.3 Graph Memory Construction\n\nWhile the EML captures the narrative flow, the GML organizes static facts into a consistent relational structure.\n\n3.3.1 Fact Extraction and Grounding\n\nFor each passage $p _ { t }$ , the system extracts a set of relational quadruples $\\kappa _ { t }$ to form a schema-agnostic knowledge graph:\n\n[Equation: $$\n\\mathcal {K} _ {t} = \\left\\{\\left(s, r, o, \\tau\\right) \\mid s, o \\in \\mathcal {E}, r \\in \\mathcal {R}, \\tau \\in \\mathcal {T} \\right\\} \\tag {4}\n$$]\n\nwhere $s$ and $o$ are entities, $r$ is the relation, and $\\tau$ denotes the temporal validity. Each node in the graph is also linked to its source passage $p$ via provenance pointers $\\rho _ { t } ^ { g m l }$ . To maintain graph integrity, we merge nodes that exceed a vector similarity threshold, bridging lexical variations across different passages.\n\n3.4 Hybrid Retrieval and Context Integration\n\nDuring inference, we integrate the structured facts from the GML with the narrative details from the EML through a multi-stage retrieval process.\n\n3.4.1 Relational Propagation and Passage Retrieval\n\nThe system initiates retrieval by extracting structured quadruples from the query $q$ to ensure structural alignment with the GML. A shared semantic encoder transforms each query-derived quadruple into a dense vector representation. The retrieval engine then computes the semantic similarity between these query vectors and the pre-indexed embeddings of the facts store within the GML using cosine similarity. By ranking these scores across the relational space, the system identifies the most relevant facts to form the initial seed set ${ \\boldsymbol { \\kappa } } _ { t o p }$ . We then execute a propagation algorithm (Haveliwala, 2002) using ${ \\boldsymbol { \\kappa } } _ { t o p }$ as the seed set to compute a distribution over graph nodes. This relational traversal identifies the set of most relevant initial passages $\\mathcal { P } _ { r e t } = \\{ p _ { 1 } , p _ { 2 } , . . . , p _ { n } \\}$ through their provenance pointers.\n\n3.4.2 Reverse Provenance Expansion\n\nInitial retrieval often suffers from context fragmentation because critical details of an event may be scattered across multiple turns that lack direct lexical overlap with the query. To solve this, we use the EML as a semantic bridge. We first retrieve the event frames associated with the initial passages: Eret = Sp∈P $\\begin{array} { r } { \\mathcal { E } _ { r e t } = \\bigcup _ { p \\in \\mathcal { P } _ { r e t } } \\Phi ( p ) } \\end{array}$ , where $\\Phi ( p )$ identifies the frames linked to passage $p$ .\n\nWe then implement the reverse provenance expansion mechanism. By accessing the aggregated provenance pointers $\\rho ^ { e m l } ( \\mathbf { e } )$ of each retrieved frame (as formed during the fusion phase in Sec-\n\ntion 3.2), we expand the evidence set to include all related passages:\n\n[Equation: $$\n\\mathcal {P} _ {f i n a l} = \\mathcal {P} _ {r e t} \\cup \\bigcup_ {\\mathbf {e} \\in \\mathcal {E} _ {r e t}} \\rho^ {e m l} (\\mathbf {e}) \\tag {5}\n$$]\n\nThis ensures that if any fragment of an event is activated, all its constituent textual supports are included in the final context, providing a complete narrative for reasoning.\n\n3.4.3 Context Synthesis\n\nThe final reasoning context $\\mathbf { C }$ is synthesized by serializing the expanded passages $\\mathcal { P } _ { f i n a l }$ , the structured event frames $\\mathcal { E } _ { r e t }$ , and the relational facts $\\kappa _ { t o p }$ . This composite context enables the LLM to resolve temporal ambiguities and maintain logical consistency by cross-referencing high-level facts with nuanced episodic evidence.\n\nFinally, the agent generates the predictive response $a$ by conditioned on the query $q$ and the synthesized context C. We model this process as a sequence generation task, where the LLM acts as a decoder $G$ that maximizes the joint probability of the output tokens:\n\n[Equation: $$\na = G (q, \\mathbf {C}) = \\arg \\max  _ {a ^ {\\prime}} \\prod_ {i = 1} ^ {| a ^ {\\prime} |} P \\left(y _ {i} \\mid y _ {<   i}, q, \\mathbf {C}; \\theta\\right) \\tag {6}\n$$]\n\nwhere $y _ { i }$ denotes the $i$ -th token of the candidate answer $a ^ { \\prime }$ , and $\\theta$ represents the parameters of the generator. By prepending the structured memory evidence directly to the input space, the model can perform integrated reasoning across both episodic and relational knowledge, ensuring that the final output is not only grounded in raw evidence but also guided by the high-level semantic structure captured during the memory construction phase.",
      "caption": "SEEM architecture overview. Sequential passages are processed by Memory Generation Agents into two layers: (1) Graph Memory with timestamped facts, fine-grained semantic edges, and schema-agnostic knowledge; (2) Episodic Memory with structured narrative synthesis, cognitive frame instantiation, and granular semantic decomposition of events.",
      "image_path": "images/2601.06411v1.jpg",
      "category": "agent_reasoning",
      "aspect_ratio": 2.84,
      "source_paper": "2601.06411v1"
    },
    {
      "id": "2601.06953v2",
      "source_context": "3. Synthesis of Competition-Level Coding Data\n\nWe adapt the feature-based synthesis framework (Wang et al., 2025) to the competitive programming domain, establishing a comprehensive methodology to construct tasks that support both SFT and RL stages. As illustrated in\n\nFigure 2, this process addresses the unique challenges of synthesis through four key steps: (i) generating novel and challenging problems (with the capacity for easy scaling in quantity); (ii) constructing diverse and comprehensive test inputs for each problem (including boundary and stress tests); (iii) producing high-quality candidate solutions; and (iv) employing a dual-verification strategy that cross-checks solutions with test cases to yield more accurate test outputs and more reliable solutions.\n\n(i) Task Generation. While EpiCoder (Wang et al., 2025) introduced feature-based synthesis for general coding tasks, we extend this approach with three key improvements specifically tailored for the complexity of competitive programming. First, instead of relying on broad definitions of features, we explicitly extract and evolve competition-related features from 10k code snippets in the TACO dataset (Li et al., 2023) using GPT-4o-0513 (detailed in Appendix B.1). Second, formulating competitive scenarios from a rich feature tree is non-trivial, as LLMs often oversimplify complex prompts into trivial cases, thereby reducing both diversity and difficulty. To address this, we adopt a two-stage process that separates feature selection from scenario formulation: first, selecting mutually consistent features for meaningful composition; and second, formulating hint-free tasks that demand genuine reasoning. This two-stage approach significantly outperforms single-step generation (Appendix Table 9). We further incorporate one-shot prompting to\n\nimprove task understanding and instruction-following. Third, we adapt the synthesis method to support multi-style task generation, covering Codeforces1-style tasks (standard input/output with narrative-rich contexts), LeetCode-style2 tasks (starter code with predefined function signatures), and AtCoder3-style tasks (concise specifications with minimal explanations), thereby enhancing task diversity. Examples of the task generation process are provided in Appendix B.2, together with difficulty estimates on generated tasks in Appendix B.3 and style comparisons in Appendix B.2.3.\n\n(ii) Test Input Generation. Obtaining sufficient and accurate test cases is a formidable challenge. Problems from competitive programming platforms often do not provide test cases, or only provide a limited number, due to platform constraints. This results in insufficient quantity, difficulty, and coverage of test cases during RL training. To address the inherent scarcity of test cases in synthesized data, we investigate two complementary methods for generating the input component of the test case. The Prompting-based method instructs the LLM to interpret the problem’s input constraints and directly generate multiple test inputs, covering both standard cases and edge-case instances. The Tool-based method leverages CYaRon4, a dedicated test\n\n- 1https://codeforces.com/\n\n- 2https://leetcode.com/\n\n- 3https://atcoder.jp/\n\n- 4https://github.com/luogu-dev/cyaron\n\ncase generation library, enabling the LLM to construct test inputs by invoking functions documented within the library after understanding the problem. For each task, we generate a set of $n$ test case inputs $[ x _ { 1 } , x _ { 2 } , \\ldots , x _ { n } ]$ . Detailed description of test input generation is provided in Appendix D, and a comparative analysis is presented in Appendix D.2.\n\n(iii) Candidate Solutions Generation. For each task, we generate multiple candidate solutions using advanced open-source reasoning LLMs, obtaining $m$ answers $[ A ^ { 1 } , A ^ { 2 } , \\ldots , A ^ { m } ]$ . We verify that each candidate solution includes a complete reasoning process and a Python code implementation, and we ensure the absence of syntax errors through static analysis methods based on Abstract Syntax Tree (AST). Filtering criteria are provided in Appendix C.1.\n\n(iv) Dual-Verification of Solutions and Test Cases. We adopt a dual-verification strategy. Step 1 of this strategy extends the principle of self-consistency (Wang et al., 2023) by applying majority voting across candidate solutions from multiple LLMs, which mitigates model-specific biases and enhances generalization, thereby yielding a reliable test output for each input. Step 2 then identifies the top-performing candidate solution by incorporating test case difficulty weighting alongside a hold-out validation set.\n\nStep 1: Verification of Test Cases via Consensus Voting. First, we establish a preliminary ground truth for each test case input. For a given input $x _ { i }$ , we execute all candidate solutions to obtain a set of outputs $\\{ y _ { i } ^ { 1 } , y _ { i } ^ { 2 } , \\ldots , y _ { i } ^ { m } \\}$ , where $y _ { i } ^ { j } = A ^ { j } ( x _ { i } )$ . A provisional ground truth output $\\hat { y } _ { i }$ is determined via majority voting:\n\n[Equation: $$\n\\hat {y} _ {i} = \\underset {y} {\\operatorname {a r g m a x}} \\sum_ {j = 1} ^ {m} \\mathbb {I} \\left(y _ {i} ^ {j} = y\\right), \\tag {1}\n$$]\n\nwhere $\\mathbb { I } ( \\cdot )$ is the indicator function. Empirical evaluation on the TACO dataset demonstrates that this voting strategy achieves $9 4 . 7 \\%$ labeling accuracy with 8 sampled solutions (see analysis in Appendix E.3 and Table 14). This yields a candidate test set $\\mathcal { T } _ { c a n d i d a t e } = \\{ ( x _ { 1 } , \\hat { y } _ { 1 } ) , \\ldots , ( x _ { n } , \\hat { y } _ { n } ) \\}$ . Crucially, we posit that not all test cases are of equal importance; boundary or edge cases are critical for robust evaluation. We therefore introduce a weighting function $w ( x _ { i } )  w _ { i }$ that assigns a higher score to more challenging test cases. The weight $w _ { i }$ is determined by combining semantic-based heuristics (e.g., assigning higher weights to boundary and stress tests explicitly requested in the prompt) and size-based metrics (e.g., input file size as a proxy for computational complexity). Detailed weighting criteria are provided in Appendix E.2.\n\nStep 2: Verification of Solutions via Weighted Evaluation and Hold-out Validation. To ensure that our selected “golden” solution generalizes beyond the generated data, we partition the candidate test set. We randomly sample a subset\n\nof $\\mathcal { T } _ { c a n d i d a t e }$ (e.g., $5 0 \\%$ ) to form a hold-out validation set, $\\mathcal { T } _ { v a l }$ . The remaining data constitutes our primary weighted test suite, $\\tau _ { g o l d e n }$ . The dual-verification process culminates in selecting the golden answer, $A _ { g o l d e n }$ . A candidate solution $A ^ { j }$ is first evaluated on $\\tau _ { g o l d e n }$ using a weighted score. The top-performing candidate, $A _ { g o l d e n } ^ { \\prime }$ , is identified as:\n\n[Equation: $$\nA _ {g o l d e n} ^ {\\prime} = \\operatorname * {a r g m a x} _ {A ^ {j} \\in \\left\\{A ^ {1}, \\dots , A ^ {m} \\right\\}} \\sum_ {\\left(x _ {i}, \\hat {y} _ {i}\\right) \\in \\mathcal {T} _ {g o l d e n}} w _ {i} \\cdot \\mathbb {I} \\left(A ^ {j} \\left(x _ {i}\\right) = \\hat {y} _ {i}\\right) \\tag {2}\n$$]\n\nThe final confirmation of $A _ { g o l d e n }$ is contingent upon its performance on the unseen hold-out set $\\mathcal { T } _ { v a l }$ . We verify that $A _ { g o l d e n } ^ { \\prime }$ also achieves the highest (or a competitively high) unweighted accuracy on $\\mathcal { T } _ { v a l }$ relative to other candidates. This additional validation step ensures that the selected solution is not merely overfitted to the specifics of the weighted test cases but demonstrates superior, generalizable correctness. The detailed algorithm is provided in Appendix E.1.\n\nWe ensure task solvability by using GPT-5 (high reasoning effort) as a proxy solver, filtering out approximately $3 6 . 9 \\%$ of tasks where it achieves a zero pass rate on the voted test cases. We posit that if such a capable model fails completely, the task likely suffers from ambiguity or underspecification. The pass rate distribution of the remaining tasks is shown in Table 18 (Appendix E.5).\n\nUltimately, this process yields a verified solution $A _ { g o l d e n }$ and a comprehensive test suite $\\tau _ { g o l d e n }$ for every generated task $q$ . We leverage these high-quality synthetic assets to drive post-training for Code LLMs: $( q , A _ { g o l d e n } )$ pairs for supervised fine-tuning, and $( q , \\mathcal { T } _ { g o l d e n } )$ for reinforcement learning via GRPO algorithm (Shao et al., 2024a).",
      "caption": "Task Generation pipeline for X-Coder. Code Snippets undergo Feature Extraction (Sorting, Math, Traversal), Evolve and Merge, Select and Thinking (selected subtree + scenario generation), producing competition-level programming Tasks.",
      "image_path": "images/2601.06953v2.jpg",
      "category": "generative_learning",
      "aspect_ratio": 3.49,
      "source_paper": "2601.06953v2"
    },
    {
      "id": "2601.07033v1",
      "source_context": "3 Method\n\n3.1 Structured Representation of Narrative Commitments\n\nTo transform intuitive narrative structures into machine-actionable logic, we formalize the relationship between foreshadowing and its resolution as an explicit system of causal commitments. We argue that a complete narrative dependency consists not just of an initial setup and a final resolution, but of a specific logical \"gate\" that governs its timing. Consequently, we propose representing each commitment as a structured Foreshadow–Trigger– Payoff (F–T–P) triple:\n\n- • Foreshadow $( F )$ : The initial setup or narrative anomaly that establishes a \"causal debt,\" implying that a future explanation or resolution is required.\n\n- • Trigger $( T )$ : The specific narrative condition or prerequisite event that must occur for the latent foreshadow to become actionable.\n\n- • Payoff $( P )$ : The concluding event that logically fulfills and resolves the commitment introduced by $F$ and activated by $T$ .\n\nThis F–T–P decomposition is essential for modeling temporal appropriateness. In The Hound of the Baskervilles ( Figure 1), the missing boot $( F )$ creates suspense, but remains dormant until the revelation of Stapleton’s tracking method $( T )$ . Only then is the explanation of the boot’s purpose $( P )$ narratively justified. By explicitly modeling the Trigger, our framework distinguishes between premature payoff (spoiling suspense) and missing payoff (logical inconsistency).\n\n3.2 Codified Foreshadow-Payoff Generation\n\nThe CFPG framework externalizes narrative causality from the model’s implicit attention weights into a codified finite-state abstraction. This allows for symbolic tracking of narrative debts with a level of precision that pure prompting cannot achieve.\n\n3.2.1 Causal State and Foreshadow Pool\n\nThroughout the generation process, CFPG maintains a dynamic Foreshadow Pool $\\begin{array} { r l } { \\mathcal { C } } & { { } = } \\end{array}$ $\\{ ( F _ { i } , T _ { i } , P _ { i } ) \\} _ { i = 1 } ^ { n }$ . This pool serves as a global state representing all unfulfilled narrative commitments. Unlike standard autoregressive models that compress history into a hidden vector, CFPG represents\n\nthe narrative state as an explicit set of resolvable predicates.\n\n3.2.2 The Codified Loop: Select, Generate, and Update\n\nAs illustrated in Figure 2, CFPG operates through an iterative Select–Generate–Update cycle:\n\nEligibility Selection via Codification At each step $t$ , for each pending commitment in the foreshadow pool $\\mathcal { C } _ { t }$ , CFPG invokes the codify function. This function acts as a logical gate that evaluates the current narrative context $X _ { t }$ against the trigger $T$ . Only those foreshadows whose triggers are satisfied are promoted to the active subset $S _ { t } = \\{ f \\in \\mathcal { C } _ { t } \\ | \\ \\mathrm { c o d i f y } ( X _ { t } , f ) = \\mathrm { T r u e } \\} .$ .\n\nGuided Continuation Given the eligible subset $S _ { t }$ , the language model is tasked with generating the next scene $y$ . Unlike vanilla generation, CFPG injects the corresponding payoffs associated with $S _ { t }$ into the model’s context as explicit narrative requirements. The generation is thus formulated as:\n\n[Equation: $$\ny \\sim p _ {\\theta} \\left(y \\mid X _ {t}, S _ {t}\\right)\n$$]\n\nThis ensures that the transition to the next scene is not merely a probabilistic continuation, but a guided fulfillment of active narrative debts under explicit narrative constraints.\n\nState Transition and Grounding Following generation, CFPG performs a state update. A verification module identifies which commitments from $S _ { t }$ were successfully realized in the text $y$ and removes them from $\\mathcal { C } _ { t }$ . Simultaneously, it extracts new narrative setups introduced in $y$ and encodes them as new F–T–P triples for $\\mathcal { C } _ { t + 1 }$ . This ensures the causal state representation remains temporally grounded as the story unfolds.",
      "caption": "The Codified Foreshadow-Payoff Generation loop. Story Prefix X_t feeds into Codified Casual State (Foreshadow Pool C_t), Eligibility Selection checks triggers, Conditional Generation (LM) produces continuation y, and State Update codifies new foreshadows and resolves commitments.",
      "image_path": "images/2601.07033v1.jpg",
      "category": "generative_learning",
      "aspect_ratio": 1.72,
      "source_paper": "2601.07033v1"
    },
    {
      "id": "2601.07055v1",
      "source_context": "3 Methodology\n\n3.1 Setup\n\nWe employ a proposer-solver self-evolution framework where both models function as search agents capable of leveraging external knowledge. Equipped with the external search engine $\\mathcal { R }$ , the proposer $\\pi _ { \\theta }$ and the solver $\\pi _ { \\phi }$ are trained to maximize their respective expected rewards:\n\n[Equation: $$\n\\begin{array}{l} \\text {P r o p o s e r}: \\quad \\mathbb {E} _ {(x, y) \\sim \\pi_ {\\theta} (\\cdot \\mid \\mathcal {R}), \\left\\{\\hat {y} _ {i} \\right\\} _ {i = 1} ^ {n} \\sim \\pi_ {\\phi} (\\cdot \\mid x, \\mathcal {R})} \\left[ r \\left(y, \\left\\{\\hat {y} _ {i} \\right\\} _ {i = 1} ^ {n}\\right) \\right], \\tag {1} \\\\ \\text {S o l v e r}: \\quad \\mathbb {E} _ {(x, y) \\sim \\pi_ {\\theta} (\\cdot \\mid \\mathcal {R}), \\hat {y} \\sim \\pi_ {\\phi} (\\cdot \\mid x, \\mathcal {R})} [ \\mathbb {I} (y = \\hat {y}) ], \\\\ \\end{array}\n$$]\n\nwhere $r$ denotes the proposer reward and I is the indicator function. In contrast to the solver’s simple outcome-based reward, the proposer reward is defined over the distribution of predicted answers (i.e., $\\{ \\hat { y } _ { i } \\} _ { i = 1 } ^ { n }$ ). If all predictions are correct, the question is considered trivial, whereas if none are correct, the question is likely too difficult for the solver. To enable self-evolution of $\\pi \\theta$ and $\\pi _ { \\phi }$ , we iteratively optimize both components in a symbiotic loop: the proposer learns to synthesize diverse and challenging questions, while the solver enhances its reasoning abilities by learning from these questions. The improved solver performance subsequently encourages the proposer to generate increasingly complex queries, forming a continuously evolving curriculum (see Figure 2). We initialize both models from the same base LLM and rely exclusively on the search tool ( $\\mathcal { R }$ ) for external knowledge. To strictly adhere to our training data-free setting, we avoid utilizing any demonstrations, questions or annotated answers in our framework.\n\n3.2 Proposer Training\n\nExisting self-evolution methods primarily target specialized domains (e.g., math, coding) to enhance LLM performance without external data (Zhao et al., 2025; Huang et al., 2025a; Chen et al., 2025). However, for open-domain question answering, we find that such methods tend to generate structurally homogeneous one-hop queries, which limits solver performance gains to simple reasoning questions. Even when equipped with a multi-turn search tool, the solver yields only marginal improvements on multi-hop queries (see Section 4). Furthermore, since the proposer reward necessitates multiple solver predictions to assess query difficulty and solvability, optimizing the proposer with standard RL algorithm like GRPO becomes highly inefficient (Shao et al., 2024). For instance, generating $m$ queries with $n$ predictions each results in $( m + 1 ) \\times n$ rollouts per prompt. Combined with the high latency of multi-turn rollouts, this scaling bottleneck renders existing approaches impractical for self-evolving agents that require complex tool interactions.\n\nMotivated by these limitations, we propose hop-grouped relative policy optimization (HRPO) to train the proposer model. Instead of sampling multiple responses for a single prompt, HRPO calculates advantages by grouping structurally similar questions. Specifically, we cluster the generated QA pairs by their cross-hop complexity, denoted by the number of hops $h \\in \\mathcal H$ . Questions with fewer hops are typically simpler, whereas higher-hop questions demand extensive search and multi-step reasoning. This hop-specific normalization of returns produces low-variance advantage estimates while avoiding the computational cost of sampling multiple candidate questions per prompt. HRPO can be formulated with:\n\n[Equation: $$\n\\begin{array}{l} \\mathcal {J} (\\theta) = \\mathbb {E} _ {\\left\\{\\left(x _ {i}, y _ {i}\\right) \\sim \\pi_ {\\theta} (\\cdot \\mid \\mathcal {R}), \\left\\{\\hat {y} _ {i, k} \\right\\} _ {k = 1} ^ {n} \\sim \\pi_ {\\phi} (\\cdot \\mid x _ {i}, \\mathcal {R}) \\right\\} _ {i = 1} ^ {N}} \\\\ \\left[ \\frac {1}{N} \\sum_ {h \\in \\mathcal {H}} \\sum_ {i \\in \\mathcal {I} _ {h}} \\log \\pi_ {\\theta} \\left(x _ {i}, y _ {i} \\mid \\mathcal {R}\\right) A _ {i, h} \\right] - \\beta \\mathbb {D} _ {K L}, \\tag {2} \\\\ \\end{array}\n$$]\n\nwhere $N$ denotes the size of the sampled batch, and $\\beta$ is the hyperparameter controlling the KL regularizer.\n\n$A _ { i , h }$ denotes the advantage of the generated QA pair $( x _ { i } , y _ { i } )$ , computed by standardizing the solver’s reward scores over all $h$ -hop questions:\n\n[Equation: $$\nA _ {i, h} = \\frac {r _ {i} - \\mathbb {E} _ {j \\in \\mathcal {I} _ {h}} \\left[ r _ {j} \\right]}{\\sqrt {\\mathbb {V} \\operatorname {a r} _ {j \\in \\mathcal {I} _ {h}} \\left[ r _ {j} \\right]} + \\delta}. \\tag {3}\n$$]\n\nFor optimal proposer performance and training efficiency, we adopt a strictly on-policy framework and omit ratio clipping. While single-response methods like REINFORCE++ reduce sampling costs, we find that a global baseline becomes unstable when processing diverse query structures. This mismatch induces high variance in policy gradients, frequently leading to training failures. In contrast, HRPO mitigates this issue by computing relative advantages among structurally aligned trajectories.\n\nTo utilize solver signals for training the proposer $\\pi \\theta$ , we design a specialized reward function to encourage both verifiability (the task must be solvable) and difficulty (the task must not be trivial). We leverage the solver’s pass rate on the generated questions as a proxy for these properties. Let $k$ denote the number of correct solutions out of $n$ sampled attempts; we penalize instances where the solver either fails completely ( $k = 0$ ) or succeeds trivially ( $k = n$ ), incentivizing the generation of questions that maximize:\n\n[Equation: $$\nr \\left(y, \\left\\{\\hat {y} _ {i} \\right\\} _ {i = 1} ^ {n}\\right) = \\mathbb {I} \\left(0 <   k <   n\\right) \\frac {n - k}{n - 1} + r ^ {f}, \\text {w i t h} k = \\sum_ {i = 1} ^ {n} \\mathbb {I} \\left(y = \\hat {y} _ {i}\\right), \\tag {4}\n$$]\n\nhere, the reward is maximized when exactly one solution is correct and decays linearly as the number of correct predictions increases. We additionally impose a format reward $r ^ { f }$ to mitigate structural degradation during complex generation. This enables the proposer to effectively learn to interleave reasoning with search, yielding QA pairs that are both well-formed and challenging.\n\n3.3 Solver Training\n\nFor solver training, we sample data pairs $( x , y )$ from the proposer $\\pi _ { \\theta }$ and optimize $\\pi _ { \\phi }$ via group relative policy optimization (GRPO) (Shao et al., 2024). By computing advantages from the empirical group statistics, GRPO reinforces valid trajectories and refines the model’s search and reasoning capabilities without requiring a separate value function:\n\n[Equation: $$\n\\begin{array}{l} \\mathcal {J} (\\boldsymbol {\\phi}) = \\mathbb {E} _ {(x, y) \\sim \\boldsymbol {\\pi} _ {\\boldsymbol {\\theta}} (\\cdot | \\mathcal {R}), \\{\\hat {y} _ {i} \\} _ {i = 1} ^ {n} \\sim \\boldsymbol {\\pi} _ {\\boldsymbol {\\phi} _ {\\mathrm {o l d}}} (\\cdot | x, \\mathcal {R})} \\\\ \\left[ \\frac {1}{n} \\sum_ {i = 1} ^ {n} \\min  \\left(\\frac {\\pi_ {\\phi} \\left(\\hat {y} _ {i} \\mid x , \\mathcal {R}\\right)}{\\pi_ {\\phi_ {\\mathrm {o l d}}} \\left(\\hat {y} _ {i} \\mid x , \\mathcal {R}\\right)} A _ {i}, \\operatorname {c l i p} \\left(\\frac {\\pi_ {\\phi} \\left(\\hat {y} _ {i} \\mid x , \\mathcal {R}\\right)}{\\pi_ {\\phi_ {\\mathrm {o l d}}} \\left(\\hat {y} _ {i} \\mid x , \\mathcal {R}\\right)}, 1 - \\epsilon , 1 + \\epsilon\\right) A _ {i}\\right) \\right] - \\beta \\mathbb {D} _ {K L}, \\tag {5} \\\\ \\end{array}\n$$]\n\nwhere the advantages are computed via reward standardization (i.e., $\\begin{array} { r } { A _ { i } = \\frac { \\mathbb { I } ( y = \\hat { y } _ { i } ) - \\mathrm { m e a n } \\left( \\left\\{ \\mathbb { I } ( y = \\hat { y } _ { i } ) \\right\\} _ { i = 1 } ^ { n } \\right) } { \\mathrm { s t d } \\left( \\left\\{ \\mathbb { I } ( y = \\hat { y } _ { i } ) \\right\\} _ { i = 1 } ^ { n } \\right) + \\delta } . } \\end{array}$ ). The optimization is driven by an outcome-based reward that solely evaluates the correctness of final predictions against the synthesized ground truth $y$ . With increasingly complex queries from the proposer, the solver is continuously motivated to refine its search and reasoning capabilities. Such interactions create a dynamic curriculum that ensures improving solver performance across diverse problem domains without external training data. The solver’s progress, in turn, pushes the proposer to synthesize more complex cases, establishing a feedback loop that steadily expands both agents’ capabilities.\n\n3.4 The Self-Evolving Dr. Zero\n\nIn summary, we introduce Dr. Zero, a scalable and effective framework that leverages data-free self-evolution to iteratively enhance both the proposer and solver (see Figure 2). In each iteration, the proposer $\\pi _ { \\theta }$ synthesizes a batch of QA pairs with heterogeneous hop structures. Utilizing solver feedback, the proposer is optimized via HRPO to produce verifiable, diverse and challenging queries. Meanwhile, the solver leverages the generated data through GRPO to refine its search and reasoning capabilities. This alternating optimization loop creates a symbiotic feedback mechanism: as the solver improves, simple queries yield diminishing rewards, forcing the proposer to explore more complex reasoning paths to maximize its returns. Conversely, the increasingly difficult questions prevent the solver’s training rewards from plateauing, allowing the solver to continuously expand its reasoning skills. Both models are initialized from the same base LLM and evolve without any training data, relying solely on the external search engine to drive their performance improvements.",
      "caption": "Self-Evolution Feedback Loop in Dr. Zero. The Proposer generates QA Pairs via Reason & Search, which the Solver attempts to solve. Predictions yield Difficulty Rewards and Outcome Rewards. Step 1 updates the Proposer via HRPO; Step 2 updates the Solver via GRPO.",
      "image_path": "images/2601.07055v1.jpg",
      "category": "agent_reasoning",
      "aspect_ratio": 2.7,
      "source_paper": "2601.07055v1"
    },
    {
      "id": "2601.09259v1",
      "source_context": "2 Methodology\n\nThe architecture is illustrated in Figure 3. In this section, we first introduce the preliminaries of LLM agents-based reasoning. We then present the three key components of MAXS: a lookahead strategy for simulating future steps, a value estimation mechanism for action scoring, and a trajectory con-\n\nvergence module that improves inference efficiency via early rollout termination.\n\n2.1 Preliminaries\n\nDefinition 1: Tool-Augmented Reasoning. LLM Agents reasoning is an iterative process where the agent generates steps $s _ { i }$ based on the reasoning history and input, including the question and prompt $s _ { 0 }$ :\n\n[Equation: $$\ns _ {i} \\sim \\pi_ {\\theta} (\\cdot \\mid s _ {0}, s _ {\\leq i - 1}), \\tag {1}\n$$]\n\nwhere $\\pi _ { \\theta }$ is the policy of a pre-trained LLM with parameters $\\theta$ , and $s { \\le } i$ denotes all previous reasoning steps. In tool-augmented settings, the agent can choose to invoke external tools (e.g., search or code) at selected steps ${ \\mathcal { T } } _ { \\mathrm { t o o l } } \\subseteq \\{ 1 , \\dots , T \\}$ to enhance reasoning. The final output $s _ { n }$ is generated by combining the input question $s _ { 0 }$ with retrieved and computed results:\n\n[Equation: $$\ns _ {n} \\sim \\pi_ {\\text {f i n a l}} \\left(s _ {0}; \\left\\{d _ {i}, r _ {i} \\right\\} _ {i \\in \\mathcal {I} _ {\\text {t o o l}}}\\right). \\tag {2}\n$$]\n\nDefinition 2: Test-Time Strategy. To improve reasoning quality, the agent may apply a selection policy $\\mathcal { Q }$ to refine the next step:\n\n[Equation: $$\n\\hat {s} _ {i} \\sim \\mathcal {Q} (\\cdot \\mid s _ {0}, s _ {\\leq i - 1}), \\tag {3}\n$$]\n\nwhere $\\hat { s } _ { i }$ is the selected optimal step, and $\\mathcal { Q }$ denotes a test-time strategy such as MCTS.\n\nDefinition 3: Search Tool Invocation. At reasoning step $i$ , the agent may generate a query to retrieve external knowledge based on input $x$ :\n\n[Equation: $$\nq _ {i} ^ {\\text {s e a r c h}} \\sim \\pi_ {\\text {s e a r c h}} \\left(s _ {0}, s _ {i}\\right), d _ {i} = \\operatorname {S e a r c h} \\left(q _ {i} ^ {\\text {s e a r c h}}\\right). \\tag {4}\n$$]\n\nThe document $d _ { i }$ is used to update the next step.\n\nDefinition 4: Code Tool Invocation. At some steps, the agent may also invoke a code tool to perform computation based on the current state and input $x$ :\n\n[Equation: $$\nc _ {i} \\sim \\pi_ {\\text {c o d e}} \\left(s _ {0}, s _ {i}\\right), r _ {i} = \\operatorname {E x e c} \\left(c _ {i}\\right). \\tag {5}\n$$]\n\nThe result $r _ { i }$ is integrated into next reasoning process.\n\n2.2 Lookahead Strategy\n\nTo mitigate the issue of locally myopic generation, we adopt lookahead via a rollout process. This approach evaluates the current step $s _ { i }$ and future steps $s _ { > i }$ to determine the most optimal decision. The lookahead process is defined as:\n\n[Equation: $$\n\\hat {s} _ {i} \\sim \\pi_ {\\theta} \\left(s _ {i} \\mid s _ {0}, s _ {<   i}, s _ {> i}\\right), \\tag {6}\n$$]\n\nwhere $s _ { i }$ is the current reasoning state, $s _ { 0 }$ represents the input question and prompt, and $s _ { > i }$ includes future steps to be evaluated.\n\nAccording to the Bellman Optimality Principle (Barron and Ishii, 1989), the value of future steps $R ( s _ { > i } )$ can be recursively estimated as:\n\n[Equation: $$\nR \\left(s _ {0}, s \\leq i, s > i\\right) = \\mathbb {E} \\left[ \\sum_ {k = 1} ^ {K} \\gamma^ {k - 1} R \\left(s _ {i + k}\\right) \\mid s \\right], \\tag {7}\n$$]\n\nwhere $\\gamma$ is the discount factor for future steps, $K$ is the maximum number of steps in the lookahead,\n\nand $s$ is the whole steps. This allows us to incorporate future trajectory values into the decisionmaking process.\n\nProposition 1 (Bellman Recursion). The optimal action at step i obeys $\\hat { s } _ { i } = \\mathrm { a r g } \\operatorname* { m a x } _ { s _ { i } } \\big [ R ( s _ { i } *$ $\\gamma \\mathbb { E } _ { s _ { > i } } V ^ { * } ( s _ { > i } ) \\big ]$ , hence the sequence’s optimum is obtained by recursively combining current utility with the future optimal value.\n\nThe detailed derivation can be found in Appendix A.1. Finally, the current step is selected based on the estimated future values $R ( s _ { > i } )$ as:\n\n[Equation: $$\n\\hat {s} _ {i} \\sim \\pi_ {\\theta} \\left(s _ {i} \\mid s _ {0}, s _ {<   i}\\right) e ^ {\\frac {R \\left(s _ {0} , s \\leq i , s > i\\right)}{\\tau}}, \\tag {8}\n$$]\n\nwhere $\\tau$ controls the diversity of the generated steps. The complete algorithm and decoding pipeline are presented in Appendix C.\n\n2.3 Value Estimation\n\nTo address trajectory instability, a composite value function evaluates candidate reasoning trajectories, incorporating advantage score, step-level variance,\n\nand slope-level variance to promote stable and consistent reasoning.\n\n(1) Advantage Score. We adopt beam search to maintain $K$ candidate paths. At each decoding step $i$ , for each path, we perform $M$ independent stochastic rollouts to simulate possible future trajectories and evaluate the expected lookahead return (Xu et al., 2025b). Let $F _ { i }$ be the foresight probability at step $i$ under the extended rollout:\n\n[Equation: $$\nF _ {i} = \\pi_ {\\theta} \\left(s _ {> i} \\mid s _ {0}, s _ {\\leq i}\\right), \\tag {9}\n$$]\n\nwhere $s _ { > i }$ denotes the future $N$ steps after $i$ . We define the global advantage as the relative improvement over the previous step:\n\n[Equation: $$\nA _ {i} = F _ {i} - F _ {i - 1}, \\quad R _ {i} ^ {\\mathrm {a d v}} = \\exp \\left(\\frac {A _ {i}}{\\tau}\\right), \\tag {10}\n$$]\n\nwhere $\\tau$ is a temperature parameter controlling sensitivity. $R _ { i } ^ { \\mathrm { { a d v } } }$ reflects the progress gained by choosing $s _ { i }$ .\n\n(2) Step-Level Variance. Inspired by Lyapunov stability theory (Shevitz and Paden, 2002), we interpret the lookahead trajectory as a discrete-time dynamical system. Let $g _ { n }$ denote the log-probability of the $n$ -th step in the lookahead segment $s _ { > i }$ ean over a rollout of length , and its variance as: $N$ $\\begin{array} { r } { \\bar { g } = \\frac { 1 } { N } \\sum _ { n = 1 } ^ { N } g _ { n } } \\end{array}$\n\n[Equation: $$\nV _ {\\text {s t e p}} = \\frac {1}{N} \\sum_ {n = 1} ^ {N} \\left(g _ {n} - \\bar {g}\\right) ^ {2}. \\tag {11}\n$$]\n\nLower $V _ { \\mathrm { s t e p } }$ reflects bounded fluctuation across future steps, indicating that the trajectory remains stable and resists erratic deviations, akin to Lyapunov-stable behavior. Accordingly, we define the step consistency reward as $R _ { i } ^ { \\mathrm { s t e p } } ~ =$ $\\exp \\left( - { \\frac { V _ { \\mathrm { s t e p } } } { \\tau } } \\right)$ , where $\\tau$ is a temperature parameter controlling sensitivity.\n\nProposition 2 (Deviation Bound). If √ $V _ { s t e p } \\leq \\varepsilon ,$ then $| g _ { n } - { \\bar { g } } | \\leq { \\sqrt { N \\varepsilon } }$ for every n. Bounding Vstep therefore constrains state fluctuations and yields Lyapunov-like stability.\n\nThe detailed derivation can be found in Appendix A.2. This variance serves as a regularizer to favor smoother forward reasoning paths.\n\n(3) Slope-Level Variance. Inspired by Lipschitz continuity in mathematical analysis (Heinonen, 2005), we measure the directional smoothness of the lookahead trajectory by evaluating local slope variations. We define the first-order difference $\\delta _ { n } = g _ { n + 1 } - g _ { n }$ . The average slope over a rollout of length $N$ is $\\begin{array} { r } { \\bar { \\delta } ~ = ~ \\frac { 1 ^ { - } } { N - 1 } \\sum _ { n = 1 } ^ { N - 1 } \\delta _ { n } } \\end{array}$ , and its variance is given by:\n\n[Equation: $$\nV _ {\\text {s l o p e}} = \\frac {1}{N - 1} \\sum_ {n = 1} ^ {N - 1} \\left(\\delta_ {n} - \\bar {\\delta}\\right) ^ {2}. \\tag {12}\n$$]\n\nLower $V _ { \\mathrm { s l o p e } }$ implies the trajectory’s local increments are uniformly bounded, resembling Lipschitz-continuous behavior that avoids abrupt changes. Accordingly, we define the slope consis-\n\ntency reward as $\\begin{array} { r } { R _ { i } ^ { \\mathrm { s l o p e } } = \\exp \\left( - \\frac { V _ { \\mathrm { s l o p e } } } { \\tau } \\right) } \\end{array}$ , where $\\tau$ controls sensitivity to local oscillations.\n\nProposition 3 (Lipschitz Bound). If $V _ { s l o p e } ~ \\leq$ $\\varepsilon _ { i }$ , then for all $m , n$ we have $| g _ { m } \\ - \\ g _ { n } | \\ \\leq$ $\\sqrt { ( N - 1 ) \\varepsilon } \\left| m - n \\right|$ . Hence bounding $V _ { s l o p e }$ limits worst-case jumps and enforces Lipschitz-like smoothness.\n\nThe detailed derivation can be found in Appendix A.3. This reward encourages the model to prefer directionally coherent forward reasoning paths.\n\nCombining Multiple Rewards. We combine the normalized scores of advantage, consistency, and slope into a unified reward:\n\n[Equation: $$\n\\begin{array}{l} R (s _ {0}, s _ {\\le i}, s _ {> i}) = (1 - \\alpha - \\beta) \\cdot \\mathrm {N o r m} (R _ {i} ^ {\\mathrm {a d v}}) \\\\ + \\alpha \\cdot \\operatorname {N o r m} \\left(R _ {i} ^ {\\text {s t e p}}\\right) + \\beta \\cdot \\operatorname {N o r m} \\left(R _ {i} ^ {\\text {s l o p e}}\\right), \\tag {13} \\\\ \\end{array}\n$$]\n\nwhere each component is temperature-scaled and normalized by $\\begin{array} { r } { \\begin{array} { l l l } { \\mathrm { N o r m } ( R _ { i } ) } & { \\stackrel { \\displaystyle \\mathrm { ~ \\tiny ~ \\hat { ~ } { ~ } ~ } } { = } } & { \\frac { \\exp ( R _ { i } / \\tau ) } { \\sum _ { j } \\exp ( R _ { j } / \\tau ) } } \\end{array} } \\end{array}$ , with $\\tau = 0 . 6$ .\n\nReplacing this formulation of $R$ into Eq. 8, the objective becomes sampling from the joint distribution that captures advantage, consistency, and directional smoothness.\n\n2.4 Trajectory Convergence\n\nTo reduce computation and improve inference efficiency, we monitor the variance of candidate rewards $R ( s _ { 0 } , s _ { \\leq i } , s _ { > i } )$ at each step. Once the\n\nvariance falls below a threshold $\\delta$ , we stop rollout and resume auto-regressive decoding. Let Ri = {R(k)(s0, s(k)≤i , s(k)>i )}Kk=1. $\\mathcal { R } _ { i } = \\{ R ^ { ( k ) } ( s _ { 0 } , s _ { \\leq i } ^ { ( k ) } , s _ { > i } ^ { ( \\bar { k ) } } ) \\} _ { k = 1 } ^ { K }$ The early stopping condition is:\n\n[Equation: $$\n\\operatorname {V a r} \\left(\\mathcal {R} _ {i}\\right) \\leq \\delta . \\tag {14}\n$$]\n\nWe terminate rollout at step $i$ and resume decoding under the auto-regressive process. For all experiments, we set the convergence threshold $\\delta = 0 . 0 0 2$ to balance efficiency and stability.",
      "caption": "MAXS architecture. An LLM Agent with Code and Search tools processes inputs through (a) Rollout & Lookahead, (b) Value Estimation combining Advantage, Step-level Variance, and Trend Slope, and (c) Integration weighting to select the best expansion.",
      "image_path": "images/2601.09259v1.jpg",
      "category": "agent_reasoning",
      "aspect_ratio": 2.18,
      "source_paper": "2601.09259v1"
    },
    {
      "id": "2601.09708v1",
      "source_context": "3. Method\n\n3.1. Problem Formulation\n\nWe first define the setting and notations. At each timestep $t$ , given a language instruction ??, the model observes a visual input $o _ { t }$ and generates an action chunk $a _ { t }$ , represented as a sequence of continuous robot control vectors (e.g., 7- or 14-DOF for single- or bimanual robots, respectively).\n\nTo address this problem, we propose Fast-ThinkAct, an efficient reasoning framework that bridges high-level\n\nplanning with low-level action execution. Our approach employs a VLM $\\mathcal { F } _ { \\theta }$ to perform reasoning in continuous latent space, integrated with an action model $\\pi _ { \\phi }$ for executable action generation. Specifically, $\\mathcal { F } _ { \\theta }$ processes observation-instruction pairs $( o _ { t } , l )$ through latent chain-of-thought (CoT) reasoning to produce a compact visual plan latent $c _ { t }$ that encapsulates the intended trajectory in visual space (Sec. 3.2). This $c _ { t }$ subsequently guides $\\pi _ { \\phi }$ to predict executable actions $a _ { t }$ (Sec. 3.3). By distilling reasoning into a continuous latent space rather than discrete text, Fast-ThinkAct achieves significantly improved inference efficiency while enhancing action performance through better preservation of spatial and visual information.\n\n3.2. Efficient Embodied Reasoning\n\nTo enable efficient embodied reasoning that meets the real-time requirements of embodied AI tasks, we aim to compress long textual CoTs into a compact set of continuous latent representations. However, compressing reasoning traces into latents is challenging, as there is no direct supervision signal in the latent space to guide what reasoning patterns should be encoded.\n\n3.2.1. Verbalizable Latent CoT by Reward Preferences\n\nTo address this challenge, we propose to perform distillation in natural language space by introducing a verbalizer LLM that decodes latents into verbalizable reasoning. This approach grounds latent learning in an interpretable textual form, ensuring that the learned latents faithfully preserve the underlying reasoning structure. Since reasoning traces generated by the teacher model $\\mathcal { F } _ { \\theta } ^ { T }$ exhibit varying quality, we adopt a preference-based learning framework that exploits reward signals from the teacher’s GRPO training to guide the latent student $\\mathcal { F } _ { \\theta }$ toward high-quality reasoning patterns while suppressing low-quality ones.\n\nSpecifically, we employ a teacher-student framework where a textual teacher model $\\mathcal { F } _ { \\theta } ^ { T }$ first learns explicit reasoning through GRPO Shao et al. (2024) training by maximizing:\n\n[Equation: $$\n\\mathcal {J} _ {\\mathrm {G R P O}} (\\theta) = \\mathbb {E} _ {\\tau \\sim \\mathcal {F} _ {\\theta} ^ {T}} \\left[ \\min  \\left(r _ {\\theta} (\\tau) A (\\tau), \\operatorname {c l i p} \\left(r _ {\\theta} (\\tau), 1 - \\epsilon , 1 + \\epsilon\\right) A (\\tau)\\right) \\right], \\tag {1}\n$$]\n\nwhere $\\tau$ denotes a reasoning trace and $\\begin{array} { r } { r _ { \\theta } ( \\tau ) = \\frac { \\mathcal { F } _ { \\theta } ^ { T } ( \\tau ) } { \\mathcal { F } _ { \\mathrm { o l d } } ^ { T } ( \\tau ) } } \\end{array}$ is the probability ratio. The advantage function for group\n\nrewards $\\{ R _ { i } \\} _ { i \\in G ( \\tau ) }$ is represented as:\n\n[Equation: $$\nA (\\tau) = \\frac {R _ {\\tau} - \\operatorname {m e a n} \\left(\\left\\{R _ {i} \\right\\} _ {i \\in G (\\tau)}\\right)}{\\operatorname {s t d} \\left(\\left\\{R _ {i} \\right\\} _ {i \\in G (\\tau)}\\right)}. \\tag {2}\n$$]\n\nThis training process produces textual CoTs with varying quality, where the advantage function $A ( \\tau )$ naturally serves as a quality indicator. To construct preference pairs for distillation, we select the highest and lowest advantage traces from each rollout group:\n\n[Equation: $$\n\\tau^ {+} = \\underset {\\tau \\in G} {\\arg \\max } A (\\tau) \\text {a n d} \\tau^ {-} = \\underset {\\tau \\in G} {\\arg \\min } A (\\tau). \\tag {3}\n$$]\n\nInstead of generating textual tokens, the student model $\\mathcal { F } _ { \\theta }$ performs latent reasoning by autoregressively generating $M$ continuous latent vectors $\\mathbf { z } = \\{ z _ { m } \\} _ { m = 1 } ^ { M }$ with $z _ { m } \\in \\mathbb { R } ^ { d }$ , where $d$ is the hidden size. We then train the verbalizer LLM $\\nu _ { \\psi }$ to decode these latents $\\mathbf { z }$ into natural language. The training objective encourages the verbalizer to assign a higher likelihood to decoding latents into high-quality reasoning $\\tau ^ { + }$ than low-quality reasoning $\\tau ^ { - }$ . Inspired by DPO Rafailov et al. (2023), we formulate this as an optimization guided by the reward preferences:\n\n[Equation: $$\n\\mathcal {L} _ {\\text {v e r b}} = - \\mathbb {E} \\left[ \\log \\sigma \\left(\\beta \\left(\\log \\frac {p _ {\\psi} \\left(\\tau^ {+} \\mid \\mathbf {z}\\right)}{p _ {\\text {r e f}} \\left(\\tau^ {+}\\right)} - \\log \\frac {p _ {\\psi} \\left(\\tau^ {-} \\mid \\mathbf {z}\\right)}{p _ {\\text {r e f}} \\left(\\tau^ {-}\\right)}\\right)\\right) \\right], \\tag {4}\n$$]\n\nwhere $p _ { \\mathrm { r e f } }$ is the reference model (i.e., $\\mathcal { V } _ { \\psi }$ without latent conditioning), $\\sigma$ is the sigmoid function, and $\\beta = 0 . 1$ controls preference strength. This encourages the student VLM $\\mathcal { F } _ { \\theta }$ to encode latents that the verbalizer decodes into high-quality reasoning while suppressing low-quality patterns.\n\n3.2.2. Action-Aligned Visual Plan Distillation\n\nWhile the verbalizer loss (Eq. 4) enables the student $\\mathcal { F } _ { \\theta }$ to capture high-level reasoning patterns, it does not explicitly ensure that latent representations encode the visual planning capability crucial for embodied control. To address this, we introduce action-aligned visual plan distillation to transfer the teacher $\\mathcal { F } _ { \\theta } ^ { T }$ ’s spatial reasoning ability to the student $\\mathcal { F } _ { \\theta }$ .\n\nWe distill spatial reasoning from the teacher, which is trained with trajectory-level rewards (e.g., goal completion and trajectory alignment Huang et al. (2025)) for grounded visual planning. We align the trajectory-level representations by minimizing the L2 distance between hidden states of the <answer> token that encodes the visual plan:\n\n[Equation: $$\n\\mathcal {L} _ {\\text {d i s t i l l}} = \\left\\| h _ {t} ^ {T} - h _ {t} \\right\\| _ {2} ^ {2}, \\tag {5}\n$$]\n\nwhere $h _ { t } ^ { T }$ and $h _ { t }$ are the hidden states from teacher (corresponding to $\\tau ^ { + }$ ) and student, respectively.\n\nTo enable efficient parallel trajectory prediction, unlike the textual teacher that autoregressively generates verbose text sequences of waypoints $\\{ p _ { k } \\} _ { k = 1 } ^ { K }$ with $p _ { k } \\in [ 0 , 1 ] ^ { 2 }$ (tokenized into 60-70 tokens when $K = 5$ ), the student uses $K$ learnable spatial tokens $\\{ \\mathbf { s } _ { i } \\} _ { i = 1 } ^ { K }$ appended to the reasoning latent sequence, with each output hidden state simultaneously projected to a waypoint via an MLP. The total objective for training $\\mathcal { F } _ { \\theta }$ combines all three components:\n\n[Equation: $$\n\\mathcal {L} _ {\\text {s t u d e n t}} = \\mathcal {L} _ {\\text {v e r b}} + \\mathcal {L} _ {\\text {d i s t i l l}} + \\mathcal {L} _ {\\text {a n s}}, \\quad \\text {w h e r e}\n$$]\n\n[Equation: $$\n\\mathcal {L} _ {\\mathrm {a n s}} = \\sum_ {i = 1} ^ {K} \\left\\| p _ {i} - \\hat {p} _ {i} \\right\\| _ {2} ^ {2}, \\text {w i t h} p _ {i} = \\operatorname {M L P} \\left(h ^ {\\prime} \\left(\\mathbf {s} _ {i}\\right)\\right), \\tag {6}\n$$]\n\nwhere $h ^ { \\prime } ( \\mathbf { s } _ { i } )$ denotes the output hidden state of the $i$ -th spatial token and $\\hat { p } _ { i }$ are ground-truth waypoints. Through this unified framework, the student model $\\mathcal { F } _ { \\theta }$ performs compact yet expressive latent reasoning and generates visual trajectory plans efficiently.\n\n3.3. Reasoning-Enhanced Policy Learning\n\nAfter the student VLM $\\mathcal { F } _ { \\theta }$ performs compact latent reasoning and generates visual trajectory planning through spatial tokens, we leverage these representations to guide a diffusion Transformer-based action model $\\pi _ { \\phi }$ (e.g., RDT Liu et al. (2024)) for action prediction. To bridge the high-level visual planning with low-level action generation, we connect the visual latent planning $c _ { t }$ encoded in the key-value cache corresponding to the spatial tokens to the action model.\n\nSpecifically, we extract visual latent planning $c _ { t }$ from the KV cache of spatial tokens in earlier VLM layers (since $\\mathcal { F } _ { \\theta }$ has more layers than $\\pi _ { \\phi }$ ) and concatenate with KV pairs from the action model’s state encoder. The action model’s cross-attention then attends to both the visual planning context and state observations. We post-train on action-annotated robot data by freezing $\\mathcal { F } _ { \\theta }$ and the state encoder while updating only $\\pi _ { \\phi }$ with the imitation learning objective:\n\n[Equation: $$\n\\mathcal {L} _ {\\mathrm {I L}} (\\phi) = \\ell \\left(\\pi_ {\\phi} \\left(o _ {t}, l, c _ {t}\\right), \\hat {a} _ {t}\\right), \\tag {7}\n$$]\n\nwhere $\\ell$ denotes the denoising objective for diffusion policy and $\\hat { a } _ { t }$ is the ground-truth action. Through this posttraining, the action model effectively translates visual planning from compact latent reasoning into low-level robot actions.\n\n3.4. Learning Strategy and Inference\n\nTraining Strategy. We initialize both teacher $\\mathcal { F } _ { \\theta } ^ { T }$ and student $\\mathcal { F } _ { \\theta }$ from the same checkpoint obtained through SFT and CoT-SFT on a pre-trained VLM. The teacher is trained with GRPO using action-aligned rewards Huang et al. (2025), while the student is trained with $\\mathcal { L } _ { \\mathrm { s t u d e n t } }$ to compress reasoning into compact latents. We then connect the trained $\\mathcal { F } _ { \\theta }$ with action model $\\pi _ { \\phi }$ (initialized from Liu et al. (2024)) by freezing $\\mathcal { F } _ { \\theta }$ and the state encoder while updating the latent projector and $\\pi _ { \\phi }$ with ${ \\mathcal { L } } _ { \\mathrm { I L } }$ on large-scale robotic data. For target environment adaptation (e.g., LIBERO Liu et al. (2023), RoboTwin2.0 Chen et al. (2025)), we fine-tune on environment-specific demonstrations.\n\nInference. The $\\mathcal { F } _ { \\theta }$ processes $( o _ { t } , l )$ by compact latent reasoning, generating visual trajectories via $K$ spatial tokens. The visual latent planning $c _ { t }$ , extracted from the spatial tokens’ KV cache, conditions $\\pi _ { \\phi }$ to predict\n\nactions $a _ { t }$ . Inference requires only $\\mathcal { F } _ { \\theta }$ and $\\pi _ { \\phi }$ ; the verbalizer $\\nu _ { \\psi }$ is used solely during training and optionally for interpretability.",
      "caption": "Fast-ThinkAct training framework. (a) A Textual Teacher generates verbose reasoning, distilled into a Latent Student producing compact latent tokens z. GRPO Rollouts with reward provide the Verbalizer LLM loss. (b) The Latent Student Spatial KV feeds into an Action Model for robotic manipulation.",
      "image_path": "images/2601.09708v1.jpg",
      "category": "vision_perception",
      "aspect_ratio": 2.75,
      "source_paper": "2601.09708v1"
    },
    {
      "id": "2601.14724v2",
      "source_context": "3 HERMES\n\nWe propose HERMES, a training-free framework that can be seamlessly integrated with MLLMs. As shown in Fig. 3, HERMES has three components: hierarchical KV cache management, cross-layer memory smoothing, and position re-indexing.\n\n3.1 Hierarchical KV Cache Management\n\nMotivated by the layer-wise attention patterns identified in Sec. 2, we design a hierarchical KV cache strategy. For each video token with KV cache index ?? at layer ??, where ?? denotes its physical position in KV cache, we compute an importance score $S _ { i } ^ { l }$ to decide its retention:\n\n• Shallow Layers: They act as sensory memory with strong recency bias. Inspired by Ebbinghaus’memory decay theory [14], we model token importance using an exponential forgetting curve based on temporal distance:\n\n[Equation: $$\nS _ {i} ^ {l} = \\alpha_ {i} ^ {l} \\cdot e ^ {- k \\Delta t _ {i}}, \\Delta t _ {i} = T - 1 - i, \\tag {1}\n$$]\n\nwhere $T$ is the total number of video tokens in the cache, $k > 0$ is the forgetting rate, $\\alpha _ { i } ^ { l }$ denotes the\n\nnormalization factor.\n\n• Deep Layers: Deep layers function as frame-level long-term memory with stable anchor tokens. Their attention distributions are sparse, and these anchor tokens consistently receive high attention across frames, making attention magnitude a reliable indicator of long-term importance. We therefore compute token importance directly from attention weights with respect to the user query. To handle unpredictable queries in streaming scenarios, we use a generic guidance prompt (see App. B) as a pseudo query. Token importance is computed as:\n\n[Equation: $$\nS _ {i} ^ {l} = \\alpha_ {i} ^ {l} \\cdot W _ {i} ^ {l}, \\tag {2}\n$$]\n\nwhere ${ W } _ { i } ^ { l }$ denotes the attention weight of the $i$ -th token at the layer ??.\n\n• Middle Layers: Middle layers serve as working memory, transitioning from recency-dominated shallow layers to attention-driven deep layers. We compute importance by interpolating recency and attention with a layer-dependent weight:\n\n[Equation: $$\n\\omega^ {l} = \\omega_ {0} - \\gamma \\cdot \\frac {l - l _ {\\text {s h o r t}}}{l _ {\\text {l o n g}} - l _ {\\text {s h o r t}}}, \\tag {3}\n$$]\n\nwhere $l _ { \\mathrm { s h o r t } }$ and $l _ { \\mathrm { l o n g } }$ denote the layer indices, with $\\omega _ { 0 } = 0 . 7 5$ and $\\gamma = 0 . 6$ . The importance score of token $i$ at layer ?? is then computed as\n\n[Equation: $$\nS _ {i} ^ {l} = \\left(1 - \\omega^ {l}\\right) A _ {i} ^ {l} + \\omega^ {l} R _ {i} ^ {l}, \\tag {4}\n$$]\n\nwhere $A _ { i } ^ { l }$ and $R _ { i } ^ { l }$ denote the normalized attention weight and recency score, respectively, computed as in Eqs. (1) and (2).\n\n3.2 Cross-Layer Memory Smoothing\n\nHierarchical KV cache management may introduce cross-layer inconsistency, as tokens at the same cache index can be evicted independently across layers, leading to misaligned visual memory. Since effective LLM memory relies on cross-layer interaction [6, 19, 33, 39], we address this issue with Cross-Layer Memory Smoothing.\n\nInstead of treating video tokens at the same KV cache index as independent across layers, we propagate and smooth importance signals from deeper to shallower layers. Given raw importance scores $S _ { i } ^ { l } ,$ the smoothed score is computed as:\n\n[Equation: $$\n\\tilde {S} _ {i} ^ {l} = \\left(1 - \\lambda_ {l}\\right) \\cdot S _ {i} ^ {l} + \\lambda_ {l} \\cdot S _ {i} ^ {l + 1}, \\tag {5}\n$$]\n\n$\\lambda \\in [ 0 , 1 ]$ is the smoothing hyperparameter that controls the strength of cross-layer smoothing.\n\nWe then apply Top-K selection based on $\\tilde { S } _ { i } ^ { l }$ to maintain a fixed memory budget $| M |$ per layer:\n\n[Equation: $$\n\\mathcal {I} _ {l} = \\operatorname {T o p K} \\left(\\tilde {S} _ {l}, | M |\\right),\n$$]\n\n[Equation: $$\nK _ {l} = K _ {l} \\left[ \\mathcal {I} _ {l} \\right], \\quad V _ {l} = V _ {l} \\left[ \\mathcal {I} _ {l} \\right]. \\tag {6}\n$$]\n\nTo preserve long-term information, evicted tokens are aggregated into a summary token per layer, which compactly encodes long-term memory and is retained in the KV cache (see App. F).\n\n3.3 Position Re-Indexing\n\nContinuous accumulation of streaming inputs causes positional indices to exceed the model’s maximum supported range, severely degrading text generation quality. To stabilize inference, we apply position reindexing, which remaps positional indices to a contiguous range $[ 0 , | M | )$ within the memory budget $| M |$ . We design two strategies:\n\nLazy Re-Indexing Re-indexing is triggered only when positional indices approach the model limit, resulting in lower computational overhead. By preserving the original positional indices of recent tokens, it prevents\n\npositional drift compared to eager re-indexing, making it well suited for streaming video understanding.\n\nEager Re-Indexing Re-indexing is performed at each compression step, maintaining strictly contiguous RoPE indices in KV cache. While this strategy stabilizes long-range visual semantics [21, 22, 46], it leads to higher computational cost due to frequent re-indexing, making it more suitable for offline videos.\n\nThe details of re-indexing implementation for 1D RoPE (LLaVA-OV) and 3D M-RoPE (Qwen2.5-VL) are illustrated in App. E.1 and App. E.2, respectively.",
      "caption": "HERMES architecture for streaming video understanding. Video chunks processed by Vision Encoder into hierarchical KV Cache: Shallow (Sensory Memory), Middle (Working Memory), Deep (Long-term Memory). Cross-Layer Smoothing and Position Re-Indexing enable real-time Streaming QA.",
      "image_path": "images/2601.14724v2.jpg",
      "category": "vision_perception",
      "aspect_ratio": 2.68,
      "source_paper": "2601.14724v2"
    },
    {
      "id": "2601.15165v2",
      "source_context": "3. The Flexibility Trap\n\nIn this section, we rigorously test whether the flexibility of arbitrary-order generation translates into higher reasoning potential. We compare two decoding modes: Arbitrary Order, which follows standard\n\ndiffusion decoding with low-confidence remasking (Nie et al., 2025; Zhao et al., 2025; Wu et al., 2025b), and AR Order, where arbitrary-order flexibility is disabled and generation is strictly constrained to left-to-right decoding. We adopt the commonly used experimental setup in prior diffusion LLM work: a maximum of 256 tokens decoded over 256 steps, a semi-autoregressive block size of 32. Sampling temperature is set to 0.6 as in (Yue et al., 2025). The prompt template follows (Zhao et al., 2025). Results under alternative temperatures and sampling configurations are deferred to Appendix B.\n\n3.1. Arbitrary Order Limits Reasoning Potential\n\nPass@k analysis. We first evaluate the reasoning potential using the Pass@k metric on three representative dLLMs: LLaDA-Instruct (Nie et al., 2025), Dream-Instruct (Ye et al., 2025), and LLaDA 1.5 (Zhu et al., 2025) on four reasoning benchmarks: GSM8K, MATH-500, HumanEval, and MBPP. As shown in Figure 3, while arbitrary order often achieves competitive performance at $k = 1$ , it exhibits a notably flatter scaling curve compared to the AR mode. As $k$ increases, the AR mode demonstrates a stronger capacity to uncover correct solutions.\n\nSolution space coverage. One might wonder whether arbitrary order explores a different solution space, albeit less efficiently, which could account for its lower reasoning potential. We test this by analyzing solution coverage at $k = 1 0 2 4$ using LLaDA-Instruct. Figure 4 shows that the problems solvable by arbitrary order are largely a subset of those solved by AR. On HumanEval, $2 1 . 3 \\%$ of problems are solved exclusively by AR, whereas the reverse is negligible (0.6%). This indicates that the flexible decoding\n\nprocess rarely unlocks genuinely new solutions. Instead, it appears to retreat into a more conservative subset of the AR solution space.\n\n3.2. Mechanism: The Entropy Degradation\n\nAdaptive decoding bypasses logical forks. To understand why the theoretically superior solution space of dLLMs collapses in practice, we examine more closely how the two decoding modes behave. In AR order, the model is constrained to strictly resolve the left-most unknown token at each step, forcing the model to confront uncertainty as it arises. In contrast, arbitrary order adaptively selects tokens to update based on model confidence, preferentially generating “easy” tokens with high certainty while bypassing “hard” ones. Inspecting the frequently bypassed tokens reveals a clear pattern: As shown in Figure 5, the diffusion sampler disproportionately defers logical connectives and transition markers such as “Therefore”, “Thus”, and “Since”. Prior work has shown that such tokens often have high entropy (which also holds true in dLLMs; see Figure 6), and act\n\nas “reasoning sparks” or “logical forks”, functioning as branching points that determine subsequent reasoning directions (Wang et al., 2025c; Cheng et al., 2025; Wang et al., 2025b; Huang et al., 2025a). In conventional language models, keeping these tokens in high-entropy state is critical for effective exploration of the reasoning space (Wang et al., 2025c).\n\nThe “entropy degradation” phenomenon. How does the adaptive behavior of arbitrary order affect these logical forking tokens? We measure the entropy of them when they are decoded in Figure 6 (more results in Appendix C). In AR order, these tokens generally maintain high entropy when decoded, indicating a relatively fruitful branching point where multiple reasoning paths remain viable (Wang et al., 2025c; Cheng et al., 2025). In contrast, arbitrary order exhibits a sharp decrease in entropy. By deferring the logical connectors, the model prioritizes generating easier future tokens before deciding the logic connections that lead to them. When the model eventually returns to fill in the bypassed connectors, the navigational freedom is notably constrained; the process acts less as an open-ended navigational decision at a fork, and more as a retrospective alignment to bridge the gap to the pre-generated conclusion. We term this phenomenon entropy degradation.\n\nConclusion. In summary, the flexibility of arbitrary order serves as a mechanism for inference-time exploitation rather than reasoning exploration. By bypassing high-uncertainty tokens, the model effectively collapses the solution space to a safer, lower-entropy path, squeezing out slightly better single-shot coherence at the expense of reasoning potential. Autoregressive ordering, by contrast, lack this bypassing capability and are therefore forced to sample directly from high-entropy distributions at logical forks. It is precisely this inability to circumvent critical decision points that prevents the premature narrowing of the search space and preserves the reasoning potential.\n\n4. “Just GRPO” for Diffusion Language Models\n\nThe findings in Section 3 suggest that arbitrary order actually limits the reasoning potential accessible to RL. Despite this, current RL methods for dLLMs remain heavily burdened by the need to preserve this specific flexibility. In this section, we uncover the heavy “tax” imposed by this flexibility (Section 4.1) and show that discarding it enables a minimalist yet surprisingly effective solution: JustGRPO (Section 4.2).\n\n4.1. The Flexibility Tax in dLLMs’ RL\n\nExisting diffusion RL methods operate under the premise that the policy must accommodate the combinatorial space of denoising trajectories $\\tau$ to preserve arbitrary order generation. This design choice, while conceptually general, introduces several fundamental challenges:\n\nAmbiguity in token-level decomposition. In dLLMs, a generation state $s _ { t }$ is a noisy sequence conditioned on a stochastic unmasking trajectory $\\tau$ . Unlike autoregressive models, dLLMs do not admit a unique, index-aligned conditional probability of the form $\\pi ( o _ { t } \\mid s _ { t } )$ , making token-level credit assignment ambiguous and rendering the standard importance ratio $\\begin{array} { r } { \\rho _ { t } = \\frac { \\pi _ { \\theta } \\left( o _ { t } \\vert s _ { t } \\right) } { \\pi _ { \\mathrm { o l d } } \\left( o _ { t } \\vert s _ { t } \\right) } } \\end{array}$ hard to define.\n\nIntractable sequence likelihood. Autoregressive models factorize the sequence likelihood as $\\log \\pi ( o ) =$ $\\begin{array} { r l } { \\sum _ { t } \\log \\pi ( o _ { t } } & { { } | \\ o _ { < t } , q ) } \\end{array}$ , whereas dLLMs require marginalization over all valid denoising trajectories, $\\begin{array} { r } { \\pi _ { \\theta } ( o \\mid q ) = \\sum _ { \\tau \\in \\mathcal { T } } \\pi _ { \\theta } ( o , \\tau \\mid q ) } \\end{array}$ . For a sequence of length $N$ , the trajectory space grows as $| \\mathcal { T } | = O ( N ! )$ rendering exact likelihood computation infeasible and forcing existing methods to rely on ELBO-based surrogates rather than the original objective (Wang et al., 2025a; Ou et al., 2025).\n\nSampler-learner mismatch. Even with an accurate likelihood approximation, a more subtle issue persists. In practice, rollout samples are typically produced by confidence-based generation $o \\sim \\pi _ { \\theta } ^ { \\mathrm { c o n f } } ( o \\mid q )$ to navigate the combinatorial space. However, the ELBO objective still targets the likelihood of the original model distribution $\\pi _ { \\boldsymbol { \\theta } } ( o \\mid q )$ , rather than that of the actual sampling policy $\\pi _ { \\theta } ^ { \\mathrm { c o n f } } ( o \\mid q )$ , leading to a mismatch between rollout and optimization that can degrade performance (Schulman et al., 2015).\n\n4.2. JustGRPO\n\nWe propose a return to simplicity. Since pure autoregressive order yields better reasoning potential (Section 3), we explicitly forgo arbitrary-order generation during the RL stage. This constraint transforms the dLLM from a chaotic sequence denoiser into a well-defined autoregressive policy $\\pi _ { \\boldsymbol { \\theta } } ^ { A R }$ .\n\nFormulation. Standard GRPO assumes a policy $\\pi ( o _ { k } | o _ { < k } , q )$ accepting a partial sequence $o _ { < k }$ with all tokens observed and predicts one token $o _ { k }$ at a time, where $q$ is the query. Diffusion language models, however, are architected as sequence-level denoisers that accept a full sequence with mixed observed and masked tokens and predict the original values for all masked positions simultaneously.\n\nBy forgoing arbitrary-order generation, we are able to bridge the above gap and rigorously define an AR policy $\\pi _ { \\theta } ^ { \\mathrm { A R } }$ for dLLMs. To obtain the probability of the next token $o _ { k }$ given history $O _ { < k }$ , we construct $\\scriptstyle { \\tilde { x } } _ { k }$ where the past is observed and the future is masked:\n\n[Equation: $$\n\\tilde {x} _ {k} = \\underbrace {\\left[ \\begin{array}{l} o _ {1} , \\dots , o _ {k - 1} \\end{array} \\right]} _ {\\text {O b s e r v e d}}, \\underbrace {\\left[ \\begin{array}{l} \\text {M A S K} \\end{array} \\right] , \\dots , \\left[ \\begin{array}{l} \\text {M A S K} \\end{array} \\right]} _ {\\text {M a s k e d}}. \\tag {5}\n$$]\n\nAlthough the diffusion language model outputs predictions for all masked positions, the autoregressive policy concerns only the next token $o _ { k }$ . We define $\\pi _ { \\theta } ^ { \\mathrm { A R } } ( \\cdot | o _ { < k } , q )$ as follows:\n\n[Equation: $$\n\\pi_ {\\theta} ^ {\\mathrm {A R}} (\\cdot | o _ {<   k}, q) \\triangleq \\operatorname {S o f t m a x} \\left(f _ {\\theta , k} \\left(\\tilde {x} _ {k}, q\\right)\\right), \\tag {6}\n$$]\n\nwhere $f _ { \\theta , k }$ denotes the model logits at position $k$ . Consequently, the likelihood of a complete reasoning chain $o$ is exactly computable as:\n\n[Equation: $$\n\\pi_ {\\theta} ^ {\\mathrm {A R}} (o | q) = \\prod_ {k = 1} ^ {| o |} \\pi_ {\\theta} ^ {\\mathrm {A R}} \\left(o _ {k} \\mid o _ {<   k}, q\\right). \\tag {7}\n$$]\n\nOptimization. The above formlanguage models. For each query $q$ lation enables the direct appli, we sample a group of outputs $\\{ o _ { i } \\} _ { i = 1 } ^ { G }$ f standard GRPO t using the old policy $\\pi _ { \\theta _ { \\mathrm { o l d } } } ^ { \\mathrm { A R } }$ usion. The\n\n[Equation: $$\n\\mathcal {J} (\\theta) = \\mathbb {E} _ {q \\sim P (Q), \\{o _ {i} \\} _ {i = 1} ^ {G} \\sim \\pi_ {\\theta_ {\\mathrm {o l d}}} ^ {\\mathrm {A R}}} \\left[ \\frac {1}{G} \\sum_ {i = 1} ^ {G} \\frac {1}{| o _ {i} |} \\sum_ {k = 1} ^ {| o _ {i} |} \\Big (\\min \\Big (\\rho_ {i, k} \\hat {A} _ {i, k}, \\mathrm {c l i p} (\\rho_ {i, k}, 1 - \\varepsilon , 1 + \\varepsilon) \\hat {A} _ {i, k} \\Big) - \\beta \\mathbb {D} _ {\\mathrm {K L}} \\Big) \\right], \\tag {8}\n$$]\n\nwhere ρi,k = $\\begin{array} { r } { \\rho _ { i , k } = \\frac { \\pi _ { \\theta } ^ { \\mathrm { A R } } ( o _ { i , k } | o _ { i , < k } , q ) } { \\pi _ { \\theta _ { \\mathrm { o l d } } } ^ { \\mathrm { A R } } ( o _ { i , k } | o _ { i , < k } , q ) } } \\end{array}$ is the probability ratio between the current and old policies and $\\hat { A } _ { i , k }$ denotes π θold the group-standardized advantage.\n\nRemarks. One might worry whether training in AR mode turns the diffusion language model into a standard autoregressive model. This is not the case. The AR constraint is applied only during training to correctly assign credit. It refines the model’s joint distribution without altering the underlying architecture. At inference time, the model retains its conditional independence properties, still allowing us to employ parallel samplers (Ben-Hamu et al., 2025) to accelerate decoding. JustGRPO thus benefits from reasoning exploration of AR order while preserving the inference speed of dLLMs (see Section 5.2).",
      "caption": "Confronting vs. bypassing uncertainty in token generation. (a) AR Order confronts uncertain (forking) tokens sequentially. (b) Arbitrary Order bypasses uncertain tokens, skipping hard decisions, which limits reasoning potential.",
      "image_path": "images/2601.15165v2.jpg",
      "category": "generative_learning",
      "aspect_ratio": 2.04,
      "source_paper": "2601.15165v2"
    },
    {
      "id": "2601.15892v2",
      "source_context": "3 Approach\n\n3.1 Efficient Knowledge Compression and Training-Inference Alignment in DLLMs\n\nMotivation. Although random masking can dramatically improve data reusability for diffusion language models [14, 41] by repeatedly revisiting the same original examples with different mask patterns applied, it imposes a substantial computational cost. Moreover, some mask patterns only demonstrate negligible training utility, as\n\nshown in recent work [26]. In particular, many masked tokens are trained in contexts where the correct answer is only weakly constrained, and the resulting gradients are dominated by noisy token co-occurrence rather than a sharp reasoning signal. Moreover, even when the model compresses knowledge efficiently under some training contexts, this does not automatically translate into good performance under the inference contexts, where a mismatch between the training and inference paradigms has been placed. We next formalize this effect and use it to motivate practical training curricula.\n\n3.1.1 Token Reasoning Knowledge under Random Masking\n\nIn the RADD [44] formulation, the concrete score at time $t$ for coordinate $i$ takes the below form:\n\n[Equation: $$\ns ^ {*} \\left(x _ {t}, t\\right) _ {\\left(i, \\hat {x} _ {i}\\right)} = \\alpha (t) p _ {0} \\left(\\hat {x} _ {i} \\mid x _ {t} ^ {\\mathrm {U M}}\\right), \\tag {5}\n$$]\n\nwhere $x _ { t } ^ { \\mathrm { U M } }$ denotes all unmasked tokens at time $t$ , and $\\alpha ( t )$ depends only on $t$ and the forward kernel. The time dependence is thus factored out, and what remains is the clean-data conditional $p _ { 0 } ( { \\hat { x } } _ { i } \\mid c )$ for various contexts $c$ induced by random masks.\n\nHowever, not every masked context can lead to a faithful reasoning unmask order towards the final answer. Consider a clean sequence:\n\n[Equation: $$\na = 1, b = 2, a + b = 3; a = 3, b = 4, a + b = 7. \\tag {6}\n$$]\n\nIf we heavily mask the middle and final parts to obtain:\n\n[Equation: $$\na = 1, b = 2, \\left[ \\mathrm {M A S K} _ {1} \\right] \\dots \\left[ \\mathrm {M A S K} _ {2} \\right] a + b = \\left[ \\mathrm {M A S K} _ {n} \\right], \\tag {7}\n$$]\n\nthe last [MASK ] cannot by itself teach the rule that $a + b$ equals the sum of the preceding a and b, because $_ n$ the model never sees clean evidence for the pair $\\textsf { a } = \\textsf { 3 }$ , b = 4. From this particular corrupted view, the model mainly learns that some number tends to appear after $a +  b =$ , and that certain pairs like $\\textsf { a } = \\textsf { 3 }$ , b $\\mathit { \\Theta } = \\mathit { \\Theta } 4$ and 7 co-occur. From the clean evidence the model has access to, the correct answer should be 3, yet the masked training signal biases the model toward predicting 7, creating a contradictory and misleading supervision signal. By contrast, in a sample like below:\n\n[Equation: $$\na = 1, b = 2, a + b = 3 \\quad \\text {m a x k e d a s} \\quad a = 1, b = 2, a + b = [ \\text {M A S K} ]. \\tag {8}\n$$]\n\nthe context strongly constrains the answer. In this way, across many such examples, the model can discover a stable mapping from evidence to output, which corresponds to an arithmetic reasoning rule rather than mere memorization.\n\nDefinition 3.1 (Token Reasoning Knowledge) Let x be a clean training sequence sampled from the real data distribution $p _ { 0 }$ , and c denote a context extracted from x that is used to predict a single token $x _ { i }$ (similarly for multiple tokens). For autoregressive training, $c _ { \\mathrm { A R } } = x _ { < i }$ , and for diffusion with random masks, $c _ { t } ^ { i } = x _ { t } ^ { \\mathrm { U M } }$ . Given a fixed context c, the clean-data conditional $p _ { 0 } ( \\cdot \\mid c )$ induces a candidate set:\n\n[Equation: $$\n\\mathcal {C} (c) = \\left\\{v \\in \\mathcal {V}: p _ {0} (v \\mid c) \\geq \\varepsilon \\right\\}, \\quad K (c) = \\left| \\mathcal {C} (c) \\right|, \\quad \\varepsilon > 0. \\tag {9}\n$$]\n\nWe define the token reasoning knowledge contained in a training example with context c as the conditional distribution of the next token restricted to its candidate set, denoted compactly by $p _ { 0 } ( \\mathcal { C } ( c ) \\mid c )$\n\nThe model’s objective is to recover this token reasoning knowledge by learning conditionals $p _ { \\theta } ( x _ { i } ^ { 0 } \\mid c )$ such that, $p _ { \\theta } ( \\mathcal { C } ( c ) \\mid c ) \\approx p _ { 0 } ( \\mathcal { C } ( c ) \\mid c )$ . Concretely, training provides randomly sampled pairs $( c , x _ { i } ^ { 0 } )$ obtained from $( x , i )$ under $p _ { 0 }$ , and the model adjusts $p _ { \\theta } ( \\cdot \\mid c )$ based on the empirical distribution of these pairs. The difficulty of learning this knowledge is analogous to a multi-class classification problem [12, 27]: (1) the size of the candidate set $K ( c )$ ; (2) how often each context–label pair $( c , x _ { i } ^ { 0 } )$ appears in the training process.\n\nWith respect to $K ( c )$ , we can distinguish three qualitative regimes:\n\n• Reasoning regime. $K ( c )$ is small and the ground-truth token $x _ { i } ^ { 0 }$ has large probability under $p _ { 0 } ( \\cdot \\mid c )$ . The mapping $c \\mapsto x _ { i } ^ { 0 }$ is almost deterministic, so repeated samples with similar $c$ provide highly aligned gradients and quickly reinforce a stable reasoning rule.\n\n- • Correlation regime. $K ( c )$ is moderate or large, and $x _ { i } ^ { 0 }$ is one of many plausible candidates with comparable probability. The model still learns that $x _ { i } ^ { 0 }$ correlates with patterns in $c$ , but samples with similar $c$ often yield different targets, so gradients partially cancel and updates primarily fit noisy co-occurrence rather than a sharp rule.\n\n- • Noise regime. $K ( c )$ is very large (on the order of $| \\nu |$ ) and $p _ { 0 } ( \\cdot \\mid c )$ is nearly flat. The context contains almost no information about $x _ { i } ^ { 0 }$ , and the model can only memorize idiosyncratic pairs $( c , x _ { i } ^ { 0 } )$ .\n\nBecause $p _ { 0 } ( \\cdot \\mid c )$ is defined with respect to the real data distribution, extremely unnatural or noisy contexts $c$ have negligible probability and can be ignored in expectation, as clarified in previous works [15, 64]. As more relevant evidence is revealed in $c$ (for example, longer and cleaner context), the conditional entropy $H ( X _ { i } \\mid c )$ cannot increase, and the candidate set size $K ( c )$ typically shrinks.\n\nFor autoregressive architectures and block diffusion with small block sizes, the context $c$ usually contains long, contiguous, and clean left-side evidence, which also matches the inherently autoregressive structure of natural language [48]. As a result, sampled contexts tend to have relatively small $K ( c )$ and allow token reasoning knowledge to be compressed efficiently. In contrast, fully bidirectional or large-block diffusion readily produces contexts similar to Eq. 7 that fall into the correlation or noise regimes, dramatically reducing the efficiency of knowledge compression.\n\nContext–label Pair $( c , x _ { i } ^ { 0 } )$ . To reliably learn the token reasoning knowledge associated with a context $c$ , the model needs a sufficient number of informative context–label pairs $( c , x _ { i } ^ { 0 } )$ . DLLMs based on random masking generate a wide variety of contexts $c$ , which greatly increases the number of context–label pairs $( c , x _ { i } ^ { 0 } )$ that the model must learn [55]. If the total training data is fixed, this diversification reduces the number of effective pairs corresponding to any specific piece of knowledge, lowering its learning efficiency and forcing the model to relearn the same underlying knowledge many more times than an AR model would require [42, 52, 55]. Although such random contexts can be viewed as a form of data augmentation, our analysis of the size of $K ( c )$ shows that many of these contexts fall into the correlation or noise regimes and therefore cannot be mapped to a clear reasoning rule. In some cases, such as the pattern in Eq. 7, the masked context $c$ even encourages learning a wrong association: the model is asked to predict a target under incomplete or misleading evidence. Such problematic contexts appear frequently when clean evidence is heavily disrupted, for example in purely bidirectional or large-block diffusion settings.\n\nMoreover, the knowledge compressed during training is only useful at inference if it is associated with contexts that actually occur at test time. Let $\\mathcal { C } _ { \\mathrm { t r a i n } }$ denote the set (or distribution) of contexts $c$ sampled during training, and let $\\it { { C } _ { \\mathrm { { i n f e r } } } }$ denote the contexts encountered along inference trajectories. Good performance requires $\\mathcal { C } _ { \\mathrm { t r a i n } }$ and $\\mathcal { C } _ { \\mathrm { i n f e r } }$ to be as close as possible. For instance, when training with block diffusion of block size $B$ and using left-to-right block-wise decoding of size $B$ at inference, the inference contexts closely match the training contexts, which makes the learned token reasoning knowledge directly applicable at test time.\n\nIn summary, to efficiently learn new knowledge while ensuring that the data augmentation induced by DLLM masking remains effective, two conditions should be satisfied: (1) the model should be exposed to clean and reliable reasoning evidence so that clear reasoning rules can be learned, and (2) the number of distinct sampled contexts $c$ should not grow excessively, and their form should align as closely as possible with the contexts encountered during inference. Based on these principles, we next design experiments to explore more suitable training pipelines.\n\n3.1.2 Curriculum Design and Empirical Results at 2.5B Scale\n\nSince initializing from an AR checkpoint has become a standard and effective practice, we also start from a 2.5B AR model trained on general-domain data, and use code data as new CPT data. Starting from this AR checkpoint, we consider the following curricula for learning new knowledge under the same compute budget, and evaluate them using block-wise decoding in the style of LLaDA [43]:\n\n(1) AR → BiDLLM: continue pure AR training on the new data, then perform CPT to a bidirectional DLLM;\n\n- (2) ARDLLM → BiDLLM: continue training with a causal-structured DLLM (AR-style diffusion), then CPT to a bidirectional DLLM;\n\n- (3) BiDLLM: directly CPT the AR checkpoint into a bidirectional DLLM and train new knowledge in that regime.\n\nAs shown in Fig. 2, we report the average performance across multiple code benchmarks. Under the same compute budget and after continued pre-training (CPT), the overall performance follows the ranking $( 1 ) > ( 2 ) > ($ 3) under the same compute.\n\nScheme (1) preserves a purely AR training structure before CPT. Under small-block decoding (block size 1 or 2), its $\\mathcal { C } _ { \\mathrm { i n f e r } }$ are almost identical to its $\\chi _ { \\mathrm { t r a i n } }$ and satisfy the two principles from the previous section: the model sees clean reasoning evidence, and the number of distinct contexts remains controlled and well aligned with small-block inference. Consequently, before CPT to BiDLLM, scheme (1) consistently achieves the best performance for small blocks. For large-block decoding (block size 32), however, scheme (1) underperforms scheme (3), because the training context distribution $\\mathcal { C } _ { \\mathrm { t r a i n } }$ of pure AR does not cover the bidirectional patterns required by large-block decoding. Formally, with causal attention, tokens later in the block cannot influence earlier ones, so when decoding an entire block at once, the generation process becomes less stable.\n\nAfter CPT into a bidirectional DLLM, scheme (1) performs well across all block sizes. This suggests that the\n\nAR phase has already compressed the new token reasoning knowledge effectively in a well-aligned context family for small blocks, and the subsequent BiDLLM CPT primarily adapts the model to the larger-block context distribution.\n\nIt is worth noting that CPT into a fully bidirectional model introduces some degradation for block-1 decoding: both pure AR and AR BiDLLM lose a similar amount of performance between 0 and 100k CPT steps at block size 1. However, the results also show that scheme (1) achieves higher learning efficiency during the AR phase, so even after the CPT-induced drop, it still attains better final performance.\n\nInterestingly, scheme (2) also achieves strong performance after CPT. In particular, after converting to BiDLLM, it outperforms scheme (3) under block-32 decoding, despite having seen a similar amount (or even less) clean evidence during training. One explanation is that the training context distribution $\\chi _ { \\mathrm { t r a i n } }$ for AR-style diffusion in scheme (2) more closely matches the prompt–response pattern commonly used at inference, where most reasoning evidence lies on the left. Thus scheme (2) enjoys better training–inference alignment than scheme (3), and this advantage persists even when both are eventually converted to a fully bidirectional DLLM.\n\nFinally, based on the above theoretical analysis and empirical results, we recommend the following training procedure:\n\n- 1. For new knowledge, first use AR training to efficiently compress it;\n\n- 2. Then perform CPT with a small-block diffusion objective, leveraging its data-augmentation properties to further improve model quality;\n\n- 3. if one wishes to explore larger block diffusion, additional CPT can be applied starting from the model obtained in step (2).\n\nIn our final system, we adopt steps (1) and (2) to train Stable-DiffCoder, as illustrated by the solid path leading to the “DLLM Base Model” in Fig. 3.\n\n3.2 Warmup for Stable DLLM Continued Pretraining\n\nPrevious work [16] has reported that CPT of mask diffusion language models (DLLMs) is highly sensitive to the learning rate. In practice, the stability of the loss and gradient norm is also affected by architecture (e.g., attention pattern, logit parametrization), numerical precision, and infrastructure details. This motivates a more robust warmup procedure for DLLM CPT.\n\nTo reduce the architectural and objective gap between AR and DLLM, we reuse the AR token head and logit-shift parametrization, and only change the attention pattern from a causal lower-triangular mask to a bidirectional, full attention mask. Even under this minimal change, naive AR DLLM CPT shows a high initial loss and large gradient-norm spikes.\n\nWe attribute the instability mainly to three factors: (i) the change of attention mask, which induces a structural distribution shift in internal representations; (ii) the higher task difficulty when the corruption process masks a large fraction of tokens, compared to AR next-token prediction; (iii) the ELBO-motivated loss weight $w ( t )$ in the DLLM objective (Eq. 4), where $w ( t )$ can be large at low masking ratios (e.g., under a linear noise schedule, masking $1 0 \\%$ of tokens yields $w ( t ) \\approx 1 0$ ). This effectively acts as loss scaling that amplifies gradient norms, making training less stable.\n\nRather than annealing the attention mask [16] (which is inconvenient for highly optimized kernels such as FlashAttention that assume a fixed mask), we warm up only the corruption process. In standard DLLM, the corruption level $t$ is sampled uniformly from $[ 0 , 1 ]$ , and the masking ratio ranges from 0 to 1. During warmup, we cap the maximum corruption level: $t \\sim \\mathcal { U } ( 0 , u _ { \\mathrm { m a x } } )$ , and linearly increase $q _ { \\mathrm { m a x } }$ from a small value $q _ { \\mathrm { i n i t } }$ (e.g., $1 0 ^ { - 3 }$ ) to 1 over $S _ { \\mathrm { w a r m u p } }$ steps:\n\n[Equation: $$\nu _ {\\mathrm {m a x}} (s) = u _ {\\mathrm {i n i t}} + (1 - u _ {\\mathrm {i n i t}}) \\cdot \\frac {s}{S _ {\\mathrm {w a r m u p}}}, \\quad s = 0, \\ldots , S _ {\\mathrm {w a r m u p}}.\n$$]\n\nThis implements a curriculum from easy (low mask ratio, almost AR-like) reconstruction to the full DLLM regime. To further suppress gradient spikes, we drop $w ( t )$ in the warmup phase and optimize\n\n[Equation: $$\n\\mathcal {L} _ {\\mathrm {D L L M}} ^ {\\mathrm {w a r m u p}} (\\theta) = - \\mathbb {E} _ {\\mathbf {x} ^ {0} \\sim p _ {\\mathrm {d a t a}}, t \\sim \\mathcal {U} (0, u _ {\\max })}, \\mathbf {x} ^ {t} \\sim q \\left(\\mathbf {x} ^ {t} \\mid \\mathbf {x} ^ {0}\\right) \\left[ \\sum_ {i = 1} ^ {N} 1 \\left[ \\mathbf {x} _ {i} ^ {t} = \\operatorname {M A S K} \\right] \\log p _ {\\theta} \\left(\\mathbf {x} _ {i} ^ {0} \\mid \\mathbf {x} _ {1: N} ^ {t}\\right) \\right]. \\tag {10}\n$$]\n\nAfter warmup, we revert to the original loss in Eq. (4) with $t \\sim \\mathcal { U } ( 0 , 1 )$ .\n\nThis warmup produces a much smoother CPT trajectory: the gradient norm spike at the AR DLLM boundary is strongly reduced, and the loss follows a characteristic “ $\\sqrt { \\cdot }$ -shaped” curve (first decreasing on easy tasks, then increasing as the mask ratio grows, and finally decreasing again). The peak loss during warmup is significantly lower than the naive AR DLLM baseline, and the final loss is comparable to AR→AR CPT. We observe similarly stable behavior when we remove the logit shift and train a block-diffusion DLLM with the same warmup (see Fig. 4).\n\n3.3 Block-wise Clipped Noise Scheduling for Masked Block Diffusion\n\nMotivation. In masked block diffusion training, at each step we corrupt only a single contiguous block of tokens rather than the full sequence. If one reuses a global continuous-time schedule $t \\in [ 0 , 1 ] \\mapsto u ( t )$ (with $q$ the corruption/mask rate) that was designed for whole-sequence diffusion, a significant portion of training steps produce weak or even zero learning signal when the block length $B$ is small. Concretely, when the corruption is applied only inside a block $\\boldsymbol { \\beta }$ of size $B$ , the expected number of corrupted tokens at time $t$ is $\\mathbb { E } [ m \\mid t ] = B u ( t )$ and the probability of observing no corrupted token is $\\operatorname* { P r } [ m = 0 \\mid t ] \\ = \\ ( 1 - u ( t ) ) ^ { B }$ .. Under a standard global linear schedule $u ( t ) = 1 - t$ with $t \\sim \\mathrm { U n i f } \\lfloor 0 , 1 \\rfloor$ , the fraction of steps with $m = 0$ equals\n\n[Equation: $$\n\\mathbb {E} _ {t} \\left[ (1 - t) ^ {B} \\right] = \\int_ {0} ^ {1} (1 - t) ^ {B} d t = \\frac {1}{B + 1}, \\tag {11}\n$$]\n\nwhich is non-negligible for typical block sizes (e.g., 1/3 for $B$ =2, $1 / 5$ for $B$ =4, $1 / 9$ for $B { = } 8$ ).\n\nInstead of redesigning the global schedule, we adopt a simple block-aware sampling rule. At each training step, after sampling $t$ , we define a block-specific mask rate by clipping $u _ { \\mathrm { b l k } } ( t ) \\ = \\ \\operatorname* { m i n } ( 1 , \\ \\operatorname* { m a x } ( u ( t ) , 1 / B ) )$ , satisfies so that $q _ { \\mathrm { b l k } } ( t ) \\in [ 1 / B , 1 ]$ $\\mathbb { E } [ m \\mid t ] = B u _ { \\mathrm { b l k } } ( t ) \\geq 1$ for all . In addition, it prevents the loss weight $t$ . This guarantees that the expected number of masked tokens in the block $\\begin{array} { r } { w ( t ) = \\frac { 1 } { u _ { \\mathrm { b l k } } ( t ) } } \\end{array}$ for those tokens from becoming excessively large, thereby promoting more stable training.\n\nGiven $u _ { \\mathrm { b l k } } ( t )$ , we independently mask each token in the chosen block $\\boldsymbol { B }$ with probability $u _ { \\mathrm { b l k } } ( t )$ , while tokens outside $\\boldsymbol { B }$ remain clean and provide context. To further ensure that every block contributes to the loss, we apply the following fallback rule: if, after sampling, no token in $\\boldsymbol { B }$ is masked (i.e., $m = 0$ ), we uniformly sample one position in $\\boldsymbol { B }$ and force it to be masked. In this way, every training step contains at least one supervised token inside the block, while still preserving the overall shape of the original schedule $u ( t )$ .",
      "caption": "Stable-DiffCoder training pipeline. AR Mode Pretraining -> Code Continuous Pretraining -> Small Block Diffusion (DLLM Mode) -> Big Block or Bidirectional Diffusion, producing AR and DLLM Base Models with four key training features.",
      "image_path": "images/2601.15892v2.jpg",
      "category": "generative_learning",
      "aspect_ratio": 0.92,
      "source_paper": "2601.15892v2"
    }
  ]
}